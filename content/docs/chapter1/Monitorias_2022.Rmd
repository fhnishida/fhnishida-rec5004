---
title: "Monitorias de Econometria I (2022)"
author: "Fábio Nishida (fabio.nishida@usp.br)"
date: "1º semestre/2022"
output: md_document
# output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("~/../OneDrive/FEA-RP/Disciplinas/REC5004_Econometria-I/Monitoria-FHN/") # OLD
# setwd("~/../FEA-RP/Disciplinas/REC5004_Econometria-I/Monitoria-FHN/") # NITRO 5
```







# Manipulação via `dplyr`
- [Vignette - Introduction to _dplyr_](https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html)
- O pacote `dplyr` facilita a manipulação dos dados por meio de funções simples e computacionalmente eficientes
- As funções pode, ser organizadas em três categorias:
    - Colunas:
        - `select()`: seleciona (ou retira) as colunas do data frame
        - `rename()`: muda os nomes das colunas
        - `mutate()`: cria ou muda os valores nas colunas
    - Linhas:
        - `filter()`: seleciona linhas de acordo com valores das colunas
        - `arrange()`: organiza a ordem das linhas
    - Grupo de linhas:
        - `summarise()`: colapsa um grupo em uma única linha
- Nesta subseção, continuaremos utilizando a base de dados de Star Wars (`starwars`), utilizada na subseção anterior.
- Você irá notar que, ao usar essas funções, o data frame é transformado em um _tibble_ que é um formato mais eficiente para tratar dados tabulares, mas que funciona de forma igual a um data frame.

```{r}
library("dplyr")
detach("package:MASS", unload = TRUE) # pode conflitar com select() deste pacote se estiver ativo

bd_sw = starwars # Dando novo nome para a base starwars
head(bd_sw)
```


## Filtre linhas com `filter()`
- Permite selecionar um subconjunto de linhas de um data frame
```yaml
filter(.data, ...)

.data: A data frame, data frame extension (e.g. a tibble), or a lazy data frame (e.g. from dbplyr or dtplyr).
...	: <data-masking> Expressions that return a logical value, and are defined in terms of the variables in .data. If multiple expressions are included, they are combined with the & operator. Only rows for which all conditions evaluate to TRUE are kept.
```
- 
```{r}
bd_sw1 = filter(bd_sw, species == "Human", height >= 100)
bd_sw1

# Equivalente a:
bd_sw[bd_sw$species == "Human" & bd_sw$height >= 100, ]
```

## Organize linhas com `arrange()`
- Reordena as linhas a partir de um conjunto de nomes de coluna
```yaml
arrange(.data, ..., .by_group = FALSE)

.data: A data frame, data frame extension (e.g. a tibble), or a lazy data frame (e.g. from dbplyr or dtplyr).
... : <data-masking> Variables, or functions of variables. Use desc() to sort a variable in descending order.
```
- Se for inserido mais de um nome de variável, organiza de acordo com a 1ª variável e, em caso de ter linhas com o mesmo valor na 1ª variável, ordena estas linhas de mesmo valor de acordo com a 2ª variável
- Para usar a ordem decrescente, temos a função `desc()`
```{r}
bd_sw2 = arrange(bd_sw1, height, desc(mass))
bd_sw2
```


## Selecione colunas com `select()`
- Seleciona colunas que são de interesse.
```yaml
select(.data, ...)

... : variables in a data frame
: for selecting a range of consecutive variables.
! for taking the complement of a set of variables.
c() for combining selections.
```
- Coloca-se os nomes das colunas desejadas para selecioná-las.
- Também é possível selecionar um intervalo de variáveis usando `var1:var2`
- Caso queira tirar apenas algumas colunas, basta informar o nome delas precedidas pelo sinal de subtração (`-var`)
```{r}
bd_sw3 = select(bd_sw2, name:eye_color, sex:species)
# equivalente: select(bd_sw2, -birth_year, -c(films:starships))
bd_sw3
```
- Note que o `select()` pode não funcionar corretamente se o pacote `MASS` estiver ativo. Caso esteja, retire a seleção do pacote `MASS` no quadrante inferior/direito em 'Packages' (ou digite `detach("package:MASS", unload = TRUE)`)
- Uma outra forma de fazer a seleção de coluna é combinando com `starts_with()` e `ends_with()`, que resulta na seleção de colunas que se iniciam e terminam com um texto dado
```{r}
head( select(starwars, ends_with("color")) ) # colunas que terminam com color
head( select(starwars, starts_with("s")) ) # colunas que iniciam com a letra "s"
```

## Renomeie colunas com `rename()`
- Renomeia colunas usando `novo_nome = velho_nome`
```yaml
rename(.data, ...)

.data: A data frame, data frame extension (e.g. a tibble), or a lazy data frame (e.g. from dbplyr or dtplyr).
...	: For rename(): <tidy-select> Use new_name = old_name to rename selected variables.
```

```{r}
bd_sw4 = rename(bd_sw3,
                haircolor = hair_color,
                skincolor = skin_color, 
                eyecolor = eye_color)
bd_sw4
```


## Modifique/Adicione colunas com `mutate()`
- Modifica uma coluna se ela já existir
- Cria uma coluna se ela não existir
```yaml
mutate(.data, ...)

.data: A data frame, data frame extension (e.g. a tibble), or a lazy data frame (e.g. from dbplyr or dtplyr).
...	: <data-masking> Name-value pairs. The name gives the name of the column in the output. The value can be:
 - A vector of length 1, which will be recycled to the correct length.
 - A vector the same length as the current group (or the whole data frame if ungrouped).
 - NULL, to remove the column.
```
```{r}
bd_sw5 = mutate(bd_sw4,
                height = height/100, # transf cm p/ metro
                BMI = mass / height^2,
                dummy = 1 # se não for vetor, tudo fica igual
                )
bd_sw5 = select(bd_sw5, BMI, dummy, everything()) # facilitar
bd_sw5
```

## Operador Pipe `%>%`
- Note que todas as funções do pacote `dyplr` anteriores têm como 1º argumento a base de dados (`.data`), e isto não é por acaso.
- O operador pipe `%>%` joga um data frame (escrito à sua esquerda) no 1º argumento da função seguinte (à sua direita).
```{r}
filter(starwars, species=="Droid") # sem operador pipe
starwars %>% filter(species=="Droid") # com operador pipe
```
- Observe que, ao usar o operador pipe, o 1º argumento com a base de dados não deve ser preenchida (já está sendo aplicada automaticamente via `%>%`).
- Note que, desde a subseção com a função `filter()` até `mutate()` fomos "acumulando" as alterações em novos data frames, ou seja, o último data frame `bd_sw5` é a base original `starwars` que foi alterada por `filter()`, `arrange()`, `select()`, `rename()` e `mutate()`.
```{r}
bd_sw1 = filter(starwars, species == "Human", height >= 100)
bd_sw2 = arrange(bd_sw1, height, desc(mass))
bd_sw3 = select(bd_sw2, name:eye_color, sex:species)
bd_sw4 = rename(bd_sw3,
                haircolor = hair_color,
                skincolor = skin_color, 
                eyecolor = eye_color)
bd_sw5 = mutate(bd_sw4,
                height = height/100,
                BMI = mass / height^2,
                dummy = 1
                )
bd_sw5 = select(bd_sw5, BMI, dummy, everything())
bd_sw5
```
- Usando o operador pipe `%>%` várias vezes, podemos ir pegando o output resultante da aplicação de uma função e jogar como input da função seguinte. Reescreveremos o código acima "em única linha" com `%>%`, chegando ao mesmo data frame de `bd_sw5`
```{r}
bd_sw_pipe = starwars %>% 
    filter(species == "Human", height >= 100) %>%
    arrange(height, desc(mass)) %>%
    select(name:eye_color, sex:species) %>%
    rename(haircolor = hair_color,
           skincolor = skin_color, 
           eyecolor = eye_color) %>%
    mutate(height = height/100,
           BMI = mass / height^2,
           dummy = 1
           ) %>%
    select(BMI, dummy, everything())
bd_sw_pipe

all(bd_sw_pipe == bd_sw5, na.rm=TRUE) # verificando se todos elementos são iguais
```

## Resuma com `summarise()`

- Podemos usar a função `summarise()` para gerar alguma estatística acerca de uma ou mais variáveis:
```{r}
starwars %>% summarise(
    n_obs = n(),
    mean_height = mean(height, na.rm=TRUE),
    mean_mass = mean(mass, na.rm=TRUE)
    )
```
- No caso acima, gerou simplesmente o tamanho da amostra e as médias de altura e de massa considerando a amostra inteira de `starwars` (o que não foi muito útil).


## Agrupe com `group_by()`
- Diferente das outras funções do `dplyr` mostradas até agora, o output do `group_by` não altera conteúdo do data frame, apenas **transforma em uma base de dados agrupada** em categorias de uma dada variável
```{r}
grouped_sw = starwars %>% group_by(sex)
class(grouped_sw)

head(starwars)
head(grouped_sw) # agrupado por sexo
```
- O `group_by()` prepara o data frame para operações que consideram várias linhas. Como exemplo, vamos criar uma coluna com a soma de `mass` de todas observações
```{r}
starwars %>%
    mutate(mean_mass = mean(mass, na.rm=TRUE)) %>% 
    select(mean_mass, sex, everything()) %>%
    head(10)
```
- Note que todos os valores de `mean_mass` são iguais. Agora, agruparemos por `sex` antes de fazer a soma:
```{r}
starwars %>%
    group_by(sex) %>%
    mutate(mean_mass = mean(mass, na.rm=TRUE)) %>% 
    ungroup() %>% # Lembre-se sempre de desagrupar depois!
    select(mean_mass, sex, everything()) %>%
    head(10)
```
- Note que, agora, a coluna `mean_mass` tem valores diferentes de acordo com o sexo da observação.
- Isso é útil em algumas aplicações econômicas em que consideramos variáveis a nível de grupo (e.g. domicílio) a qual uma observação (e.g. morador) pertence.

> **Evite potenciais erros**: Sempre que usar `group_by()`, não se esqueça de desagrupar o data frame via função `ungroup()` após realizar a operações desejadas.

## Resuma em grupos com `group_by()` e `summarise()`
- A função `summarise()` é de fato útil quando combinada com a função `group_by()`, pois conseguimos obter as estatísticas de grupos:
```{r}
summary_sw = starwars %>% group_by(sex) %>%
    summarise(
        n_obs = n(),
        mean_height = mean(height, na.rm = TRUE),
        mean_mass = mean(mass, na.rm = TRUE)
    )
summary_sw
class(summary_sw) # ao usar summary, deixa de ser agrupada
```
- Note que, ao usar `summarise()`, o data frame resultante não é agrupado e, portanto, não é necessário usar `ungroup()` neste caso.
- Também é possível adicionar mais de uma variável para agrupar:
```{r}
starwars %>% group_by(sex, hair_color) %>%
    summarise(
        n_obs = n(),
        mean_height = mean(height, na.rm = TRUE),
        mean_mass = mean(mass, na.rm = TRUE)
    )
```
- Para agrupar variáveis **contínuas**, precisamos definir intervalos usando a função `cut()`
```yaml
cut(x, ...)

x: a numeric vector which is to be converted to a factor by cutting.

breaks: either a numeric vector of two or more unique cut points or a single number (greater than or equal to 2) giving the number of intervals into which x is to be cut.
```
```{r}
# breaks com um integer = qtd desejada de grupos
starwars %>% group_by(cut(birth_year, breaks=5)) %>%
    summarise(
        n_obs = n(),
        mean_height = mean(height, na.rm = TRUE)
    )

# breaks com um vetor = quebras dos intervalos dos grupos
starwars %>% group_by(birth_year=cut(birth_year, 
                          breaks=c(0, 40, 90, 200, 900))) %>%
    summarise(
        n_obs = n(),
        mean_height = mean(height, na.rm = TRUE)
    )
```
- Note que inserimos `birth_year=cut(birth_year, ...)` para que o nome da coluna ficasse `birth_year`, caso contrário a coluna ficaria com o nome `cut(birth_year, ...)`.


## Junte bases de dados com funções _join_
- Vimos anteriormente que podemos usar o `cbind()` juntar um data frame com outro data frame (ou vetor), caso tenham o mesmo número de linhas
- Para juntar linhas (considerando que as colunas possuem as mesmas classes de variáveis), podemos usar o `rbind`
- Para agrupar bases de dados a partir de variáveis-chave, usamos a função `merge()`.
- O pacote `dplyr` fornece uma família de funções _join_ que executam o mesmo comando que `merge()`, porém, ao invés de alterar o valor de um argumento, você precisa escolher uma das funções _join_ que podem ser resumidas na seguinte figura:

```{r, echo=FALSE}
# Define variable containing url
url = "https://fhnishida.github.io/fearp/eco1/dplyr-data-join-functions.png"
```
<center><img src="`r url`"></center>

- Todas as funções possuem a mesma sintaxe:
    - `x`: base 1
    - `y`: base 2
    - `by`: vetor de variáveis-chave
    - `suffix`: vetor de 2 sufixos para incluir em colunas de mesmos nomes
- Como exemplo, usaremos subconjuntos da base de dados `starwars`:
```{r}
bd1 = starwars[1:6, c(1, 3, 11)]
bd2 = starwars[c(2, 4, 7:10), c(1:2, 6)]
bd1
bd2
```
- Note que há 12 personagens únicos em ambas bases, mas apenas "C-3PO" e "Darth Vader" são observações comuns.
- `inner_join()`: mantém apenas ID's presentes simultaneamente em ambas bases
```{r}
inner_join(bd1, bd2, by="name")
```

- `full_join()`: mantém todas ID's, mesmo que estejam em apenas em um das bases
```{r}
full_join(bd1, bd2, by="name")
```
- `left_join()`: mantém apenas ID's presentes na base 1 (informada como `x`)
```{r}
left_join(bd1, bd2, by="name")
```
- `right_join()`: mantém apenas ID's presentes na base 2 (informada como `y`)
```{r}
right_join(bd1, bd2, by="name")
```

- Note que podemos incluir mais de uma variável-chave para correspondência entre ID's de ambas bases. Primeiro, vamos construir as bases como paineis
```{r}
bd1 = starwars[1:5, c(1, 3)]
bd1 = rbind(bd1, bd1) %>%
    mutate(year = c(rep(2021, 5), rep(2022, 5)),
           # Se não for ano 2021, multiplica por um número aleatório ~ N(1, 0.025)
           mass = ifelse(year == 2021, mass, mass*rnorm(10, 1, 0.025))) %>%
    select(name, year, mass) %>%
    arrange(name, year)
bd1

bd2 = starwars[c(2, 4, 7:9), 1:2]
bd2 = rbind(bd2, bd2) %>%
    mutate(year = c(rep(2021, 5), rep(2022, 5)),
           # Se não for ano 2021, altura cresce 2%
           height = ifelse(year == 2021, height, height*1.02)) %>%
    select(name, year, height) %>%
    arrange(name, year)
bd2
```
- Note agora que, para cada personagem, temos 2 linhas que correspondem aos dois anos (2021 e 2022). Faremos um `full_join()` considerando como variáveis-chave ambos `name` e `year`.
```{r}
# Juntando as bases
full_join(bd1, bd2, by=c("name", "year"))
```
- Atente-se também aos nomes das variáveis, pois ao juntar bases com variáveis de mesmos nomes (que não são usadas como chave), a função acaba incluindo ambas variáveis renomeadas, por padrão, com sufixos `.x` e `.y` (sufixos podem ser alterados pelo argumento `suffix`)
```{r}
bd2 = bd2 %>% mutate(mass = rnorm(10)) # Criando uma variável mass

full_join(bd1, bd2, by=c("name", "year"))
```


# Gráficos
- [Exploratory graphs - Part 1 (John Hopkins/Coursera)](https://www.coursera.org/learn/exploratory-data-analysis/lecture/ilRAK/exploratory-graphs-part-1)
- [Base plotting system - Part 2 (John Hopkins/Coursera)](https://www.coursera.org/learn/exploratory-data-analysis/lecture/m4P1I/base-plotting-system-part-2)
- [Base plotting demonstration (John Hopkins/Coursera)](https://www.coursera.org/learn/exploratory-data-analysis/lecture/yUFDH/base-plotting-demonstration)


- [Aplicações R Base (The R Graph Gallery)](https://r-graph-gallery.com/base-R.html)
- Objetivos dos gráficos em análise de dados:
    1. Entender as propriedades dos dados
    2. Encontrar padrões nos dados
    3. Sugerir estratégias de modelagem
    4. Analisar "bugs"
    5. Comunicar resultados

## Gráficos para Análise Exploratória de Dados (EDA)
- Os gráficos para análise exploratória abrangem os 4 primeiros objetivos, ou seja, não são para comunicar um resultado final do seu trabalho.
- Características:
    1. Feitas rapidamente e em grande quantidade
    2. O objetivo é o entendimento dos dados
    3. Eixos/legendas normalmente são retiradas
    4. Cores/tamanhos são primariamente usadas para informação
- Principais gráficos simples:
    a. Diagrama de caixa (_Boxplot_)
    b. Histogramas
    c. Gráfico de barra (_Barplot_)
    d. Gráfico de dispersão (_Scatterplot_)

Como exemplo, usaremos dados da Agência de Proteção Ambiental dos EUA (EPA), [avgpm25.csv](https://fhnishida.github.io/fearp/eco1/avgpm25.csv), que informa a quantidade de poluição por partícula fina (PM2.5). A média anual de PM2.5 que não pode exceder 12 $\mu g/m^3$. 

```{r}
library(dplyr)
pollution = read.csv("https://fhnishida.github.io/fearp/eco1/avgpm25.csv")
summary(pollution)
```

### Diagrama de caixa (_Boxplot_)
- Apresenta mínimo, máximo, os quartis e outliers.
```{r}
boxplot(pollution$pm25, col="blue")
abline(h=12, col="red") # Linha horizontal no valor 12
```
- Para múltiplos boxplots, usamos `<variável_numérica> ~ <variável categórica>`:

```{r}
boxplot(pollution$pm25 ~ pollution$region, col="blue")
abline(h=12, col="red") # Linha horizontal no valor 12
```



### Histograma
```{r}
hist(pollution$pm25, col="green")

hist(pollution$pm25, col="green", breaks=100) # 100 quebras
rug(pollution$pm25) # Traços dos valores da amostra abaixo do histograma 
abline(v=12, col="red") # Linha vertical no valor 12
```
- Podemos colocar mais de um gráfico numa figura usando a função `par(mfrow, mar)`:
```{r}
par(mfrow=c(2, 1), mar=c(4, 4, 2, 1)) # Criando figura com 2 linhas e 1 coluna + margens

pol_west = pollution %>% filter(region == "west")
pol_east = pollution %>% filter(region == "east")

hist(pol_west$pm25, col="green")
hist(pol_east$pm25, col="green")
```

- Note que você precisa usar `par(mfrow=c(1, 1))` para voltar a incluir apenas 1 gráfico na figura. 


### Gráfico de barra (_Barplot_)
```{r}
barplot(table(pollution$region), col="wheat",
        main="Nº de países em cada região")
```



### Gráfico de dispersão (_Scatterplot_)
- Gera gráficos sob 2 dimensões
```{r}
plot(pollution$latitude, pollution$pm25)
abline(h=12, lwd=1.5, lty=2, col="red")
abline(lm(pm25 ~ latitude, data=pollution), col="blue")
```

```{r}
par(mfrow=c(1, 2), mar=c(4, 4, 2, 1)) # Criando figura com 1 linha e 2 colunas + margens

plot(pol_west$latitude, pol_west$pm25, main="West")
plot(pol_east$latitude, pol_east$pm25, main="East")
```

- Também é possível adicionar objetos gráficos e textos no gráfico gerado por `plot()`:
    - `abline()`: adiciona linhas horizontal, vertical ou de regressão
    - `points()`: adiciona pontos ao gráfico
    - `lines()`: adiciona linhas ao gráfico
    - `text()`: adiciona texto ao gráfico
    - `title()`: adiciona anotações aos eixos, título, subtítulo e margem exterior
    - `mtext()`: adiciona texto às margens (interna e externa) do gráfico
    - `axis()`: adiciona traços/rótulos aos eixos

```{r}
par(mfrow=c(1, 1)) # Retornando ao padrão

air_may = airquality %>% filter(Month==5)
air_other = airquality %>% filter(Month!=5)

plot(airquality$Wind, airquality$Ozone, main="Ozone and Wind in NYC")
points(air_may$Wind, air_may$Ozone, col="blue")
points(air_other$Wind, air_other$Ozone, col="red")
legend("topright", pch=1, col=c("blue", "red"), legend=c("May", "Other Months"))
```


Alguns parâmetros gráficos importantes:

- `pch`: símbolo dos pontos gráficos (padrão é círculo)
- `lty`: tipo da linha (padrão é linha sólida, mais pode ser pontilhado, etc.)
- `lwd`: grossura da linha (integer)
- `col`: cor, especificada como número, texto ou código hex (função `colors()` dá um vetor de cores por nome)
- `xlab`: rótulo do eixo x
- `ylab`: rótulo do eixo y
- `par()`: função que especifica parâmetros *globais* que afetam todas figuras:
    - `las`: orientação dos rótulos 
    - `bg`: cor de fundo
    - `mar`: tamanho da margem
    - `oma`: tamanho da margem externa (padrão é 0)
    - `mfrow`: número de gráficos por linha
    - `mfcol`: número de gráficos por coluna



## Grammar of Graphics (pacote `ggplot2`)
- [_ggplot2_ - Part 3 (John Hopkins/Coursera)](https://www.coursera.org/learn/exploratory-data-analysis/lecture/idcsq/ggplot2-part-3)
- [_ggplot2_ - Part 4 (John Hopkins/Coursera)](https://www.coursera.org/learn/exploratory-data-analysis/lecture/cj6RA/ggplot2-part-4)
- [_ggplot2_ Cheat Sheet](https://raw.githubusercontent.com/rstudio/cheatsheets/main/data-visualization.pdf)
- [Aplicações _ggplot2_ (The R Graph Gallery)](https://r-graph-gallery.com/ggplot2-package.html)

<!-- > ``In brief, the grammar tells us that a statistical graphic is a **mapping** from data to **aesthetic** attributes (colour, shape, size) of **geometric** objects (points, lines, bars). The plot may also contain statistical transformations of the data and is drawn on a specific coordinate system'' (from _ggplot2_ book) -->

- Componentes básicos do `ggplot2`:
    - um **data frame**
    - estética (**aesthetics**): como os dados são mapeados (tamanho, forma, cor)
    - objetos geométricos (**geoms**): pontos, linhas, formas
    - **facets**: para gráficos condicionais
- Ao invés de criar um gráfico diretamente, os gráficos do `ggplot2` são construídos em camadas (layers)

<!-- - transformações estatísticas (**stats**): como quantis, suavizamento, etc. -->
<!-- - escalas (**scales**): qual escala um mapa estético usa (exemplo: homem = vermelho, mulher = azul) -->
<!-- - sistema de coordenada (para mapas) -->

1. Data Frame
```{r}
head(mtcars)
```

2. Base do Gráfico (`ggplot()`)
    - dados que serão incluídos no gráfico
    - toda vez que for incluir uma variável, é necessário precisa usar a função `aes()` sobre elas
```{r}
library(ggplot2)
g = ggplot(data=mtcars, aes(mpg, wt)) # Criando a base do gráfico
g
```

3. Layer geomético (`geom`)
    - incluindos formas, linhas e pontos
    - caso não sejam informadas novas variáveis, a função para criar um objeto geométrico irá usar as variáveis-base definidas na função `ggplot()` inicial
    - Junta-se a base do gráfico com outras layers usando o sinal `+`
```{r}
g + geom_point()
```

4. Layer de suavização/tendência (`smooth`)
```yaml
geom_smooth(
  mapping = NULL, data = NULL, ...,
  method = NULL, formula = NULL, se = TRUE, level = 0.95
)

mapping: Set of aesthetic mappings created by aes() or aes_(). If specified and inherit.aes = TRUE (the default), it is combined with the default mapping at the top level of the plot. You must supply mapping if there is no plot mapping.
data: The data to be displayed in this layer. If NULL, the default, the data is inherited from the plot data as specified in the call to ggplot().
method: Smoothing method (function) to use, accepts either NULL or a character vector, e.g. "lm", "glm", "gam", "loess" or a function (...).
formula: Formula to use in smoothing function, eg. y ~ x, y ~ poly(x, 2), y ~ log(x).
se: Display confidence interval around smooth? (TRUE by default, see level to control.)
level: Level of confidence interval to use (0.95 by default).
```

```{r}
g + geom_point() + geom_smooth(method="lm") # suavização a partir de OLS
```



5. Layers condicionais
Facets (usar `cyl`)

```{r}
g + geom_point() + geom_smooth(method="lm") + facet_grid(. ~ cyl) # agrupando por nº cilindros horizontalmente
g + geom_point() + geom_smooth(method="lm") + facet_grid(cyl ~ .) # agrupando por nº cilindros verticalmente
```

6. Anotações
    - Rótulos: `xlab()`, `ylab()`, `labs()`, `ggtitle()`
    - Cada _geom_ tem opções para modificar, mas use `theme()` para opções globais do gráfico. Use `?theme` e veja a quantidade de configurações você pode fazer no seu gráfico.
    - Se quiser temas pré-definidos, há 2 templates padrão `theme_gray()` e `theme_bw()` (preto/branco). Também é possível usar outros usando o pacote `ggthemes`.
```{r}
g + geom_point() + ggthemes::theme_economist() + 
    ylab("Peso (libras)") + xlab("Milhas por galão") +
    ggtitle("Milhas por galão X Peso do carro")
```


7. Modificando Estética
    - Dentro de cada _geom_, podemos definir a cor (`color`), o tamanho (`size`) e a transparência (`alpha`)
```{r}
g + geom_point(color="steelblue", size=9, alpha=0.4)
g + geom_point(aes(color=cyl), size=9, alpha=0.4) # colorindo por variável - precisa usar aes()
```

# ===========================

# Distribuições
- [Probability Distributions in R (Examples): PDF, CDF & Quantile Function (Statistics Globe)](https://statisticsglobe.com/probability-distributions-in-r)
- [Basic Probability Distributions in R (Greg Graham)](https://rstudio-pubs-static.s3.amazonaws.com/100906_8e3a32dd11c14b839468db756cee7400.html)

- As funções relacionadas a distribuições são dadas por `<prefixo><nome da distribuição>`
- Existem 4 prefixos que indicam qual ação será realizada:
    - `d`: calcula a densidade da distribuição dada uma estatística
    - `p`: calcula a probabilidade acumulada dada uma estatística
    - `q`: calcula a estatística da distribuição (quantil) dada uma probabilidade 
    - `r`: gera números aleatórios dada a distribuição
- Existem diversas distribuições disponíveis no R:
    - `norm`: Normal
    - `bern`: Bernoulli (pacote `Rlab`)
    - `binom`: Binomial
    - `pois`: Poisson
    - `chisq`: Qui-Quadrado ($\chi^2$)
    - `t`: t-Student
    - `f`: F
    - `unif`: Uniforme
    - `weibull`: Weibull
    - `gamma`: Gamma
    - `logis`: Logística
    - `exp`: Exponencial
- Seguem as principais distribuições e suas respectivas funções:

| **Distribuição**   | **Densidade de Probabilidade** | **Distribuição Acumulada** | **Quantil**             |
|--------------------|--------------------------------|----------------------------|-------------------------|
| Normal             | `dnorm(x, mean, sd)`           | `pnorm(q, mean, sd)`       | `qnorm(p, mean, sd)`    |
| Binomial           | `dbinom(x, size, prob)`        | `pbinom(q, size, prob)`    | `qbinom(p, size, prob)` |
| Poisson            | `dpois(x, lambda)`             | `pnorm(q, lambda)`         | `qnorm(p, lambda)`      |
| Qui-Quadrado       | `dchisq(x, df)`                | `pchisq(q, df)`            | `qchisq(p, df)`         |
| t-Student          | `dt(x, df)`                    | `pt(q, df)`                | `qt(p, df)`             |
| F                  | `df(x, df1, df2)`              | `pf(q, df1, df2)`          | `qf(p, df1, df2)`       |

em que `x` e `q` são estatísticas de cada distribuição (quantis), e `p` é probabilidade.


## Distribuição Normal
- Criaremos gráficos com média $\mu = 0$ e desvio padrão $\sigma=1$
- Então, neste caso, os quantis em `x` e `q` são escores padrão Z
```{r}
# Gerando sequências de valores de escores Z e de probabilidades
Z = seq(-3.5, 3.5, by=0.1)
probs = seq(0.001, 0.999, by=0.001)

# Calculando densidade, distribuição acumulada e quantis
pdf_norm = dnorm(Z, mean=0, sd=1)
cdf_norm = pnorm(Z, mean=0, sd=1)
qt_norm = qnorm(probs, mean=0, sd=1)

# Gerando gráficos
plot(Z, pdf_norm, type="l", col="blue", xlab="Escores padrão Z") # pdf
plot(Z, cdf_norm, type="l", col="blue", xlab="Escores padrão Z") # cdf
plot(probs, qt_norm, type="l", col="blue", ylab="Escores padrão Z") # quantis
```

## Distribuição Binomial
- Criaremos gráficos com taxa de sucesso de 50\% (`prob = 0.5`) e número de tentativas `size = 100`
- Os quantis `x` e `q` são quantidades de sucessos
```{r}
# Gerando sequências de valores de quantis e de probabilidades
quantis = seq(35, 65, by=1)
probs = seq(0.001, 0.999, by=0.001)

# Calculando densidade, distribuição acumulada e quantis
pdf_binom = dbinom(quantis, size=100, prob=0.5)
cdf_binom = pbinom(quantis, size=100, prob=0.5)
qt_binom = qbinom(probs, size=100, prob=0.5)

# Gerando gráficos
plot(quantis, pdf_binom, type="l", col="blue", xlab="Sucessos") # pdf
plot(quantis, cdf_binom, type="l", col="blue", xlab="Sucessos") # cdf
plot(probs, qt_binom, type="l", col="blue", ylab="Sucessos") # quantis
```

## Distribuição de Poisson
- Criaremos gráficos com $\lambda = 2.5$ 
- Os quantis `x` e `q` são números de ocorrências
```{r}
# Gerando sequências de valores de quantis e de probabilidades
quantis = seq(0, 9, by=1)
probs = seq(0.001, 0.999, by=0.001)

# Calculando densidade, distribuição acumulada e quantis
pdf_pois = dpois(quantis, lambda=2.5)
cdf_pois = ppois(quantis, lambda=2.5)
qt_pois = qpois(probs, lambda=2.5)

# Gerando gráficos
plot(quantis, pdf_pois, type="l", col="blue", xlab="Ocorrências") # pdf
plot(quantis, cdf_pois, type="l", col="blue", xlab="Ocorrências") # cdf
plot(probs, qt_pois, type="l", col="blue", ylab="Ocorrências") # quantis
```


## Distribuição Qui-Quadrado
- Criaremos gráficos com 10 graus de liberdade (`df = 10`)
- Os quantis `x` e `q` são estatísticas de teste cumulativas de Pearson ($\chi^2$)
```{r}
# Gerando sequências de valores de quantis e de probabilidades
quantis = seq(1, 30, by=0.1)
probs = seq(0.001, 0.999, by=0.001)

# Calculando densidade, distribuição acumulada e quantis
pdf_chisq = dchisq(quantis, df=10)
cdf_chisq = pchisq(quantis, df=10)
qt_chisq = qchisq(probs, df=10)

# Gerando gráficos
plot(quantis, pdf_chisq, type="l", col="blue", xlab=expression(chi^2)) # pdf
plot(quantis, cdf_chisq, type="l", col="blue", xlab=expression(chi^2)) # cdf
plot(probs, qt_chisq, type="l", col="blue", ylab=expression(chi^2)) # quantis
```


## Distribuição t-Student
- Criaremos gráficos com 10 graus de liberdade (`df = 10`)
- Os quantis `x` e `q` são estatísticas _t_
- Quanto maior os graus de liberdade, mais se aproxima de uma normal (0, 1)

```{r}
# Gerando sequências de valores de quantis e de probabilidades
quantis = seq(-4.1437, 4.1437, length=100)
probs = seq(0.001, 0.999, by=0.001)

# Calculando densidade, distribuição acumulada e quantis
pdf_t = dt(quantis, df=10)
cdf_t = pt(quantis, df=10)
qt_t = qt(probs, df=10)

# Gerando gráficos
plot(quantis, pdf_t, type="l", col="blue", xlab="Estatística t") # pdf
plot(quantis, cdf_t, type="l", col="blue", xlab="Estatística t") # cdf
plot(probs, qt_t, type="l", col="blue", ylab="Estatística t") # quantis
```



## Distribuição F
- Criaremos gráficos com 10 e 15 graus de liberdade (`df1 = 10` e `df2 = 10`)
- Os quantis `x` e `q` são estatísticas _F_
```{r}
# Gerando sequências de valores de quantis e de probabilidades
quantis = seq(0.1230193, 6.080778, length=100)
probs = seq(0.001, 0.999, by=0.001)

# Calculando densidade, distribuição acumulada e quantis
pdf_f = df(quantis, df1=10, df2=15)
cdf_f = pf(quantis, df1=10, df2=15)
qt_f = qf(probs, df1=10, df2=15)

# Gerando gráficos
plot(quantis, pdf_f, type="l", col="blue", xlab="Estatística F") # pdf
plot(quantis, cdf_f, type="l", col="blue", xlab="Estatística F") # cdf
plot(probs, qt_f, type="l", col="blue", ylab="Estatística F") # quantis
```


# Estimação de Modelo Linear

É necessário carregar o pacote `dplyr` para manipulação da base de dados abaixo.
```{r message=FALSE, warning=FALSE}
library(dplyr)
```


## Base de dados `mtcars`

Usaremos dados extraídos da _Motor Trend_ US magazine de 1974, que analisa o
consumo de combustível e 10 aspectos técnicos de 32 automóveis.

No _R_, a base de dados já está incorporada ao programa e pode ser acessada pelo
código `mtcars`, contendo a seguinte estrutura:

> - _mpg_: milhas por galão
> - _cyl_: número de cilindros 
> - _disp_: deslocamento do motor
> - _hp_: cavalos-vapor bruto
> - _drat_: razão eixo traseiro
> - _wt_: peso (1000 libras)
> - _qsec_: tempo de 1/4 de milha
> - _vs_: motor (0 = forma de V, 1 = reto)
> - _am_: transmissão (0 = automático, 1 = manual)
> - _gear_: número de marchas


Façamos um resumo da base de dados:

```{r}
### Examinaremos a base da dados mtcars
## Estrutura de mtcars
str(mtcars)

## Selecionando 3 variáveis e resumindo 
mtcars %>% 
  select(mpg, hp, wt) %>% 
  summary()

## Plotando consumo de combustível (mpg) por potência do carro (hp)
plot(mtcars$mpg, mtcars$hp, xlab="Milhas por galão (mpg)", ylab="Cavalos-vapor (hp)")

## Plotando consumo de combustível (mpg) por peso do carro (wt)
plot(mtcars$mpg, mtcars$wt, xlab="Milhas por galão (mpg)", ylab="Libras (wt)")
```

Queremos estimar o seguinte modelo:
$$ \text{mpg} = \beta_0 + \beta_1 \text{hp} + \beta_2 \text{wt} + \varepsilon  $$


## Estimação por OLS

### Usando a função `lm()`

```
lm(formula, data, subset, weights, na.action,
   method = "qr", model = TRUE, x = FALSE, y = FALSE, qr = TRUE,
   singular.ok = TRUE, contrasts = NULL, offset, ...)
   
formula: an object of class "formula" (or one that can be coerced to that class): a symbolic description of the model to be fitted.
data: an optional data frame, list or environment (or object coercible by as.data.frame to a data frame) containing the variables in the model.
weights: an optional vector of weights to be used in the fitting process. Should be NULL or a numeric vector.
```
- Em `formula` você irá inserir a variável dependente separada por `~` das variáveis independentes. Estas serão separadas por um `+`. Neste exemplo: `formula = mpg ~ hp + wt`
  - Para incluir uma interação entre variáveis independentes, usa-se `<var1>:<var2>`. Por exemplo: `formula = mpg ~ hp + wt + hp:am`
  - Para fazer uma alteraçao em alguma variável dentro da função `lm()`, usa-se `I()`. Por exemplo, para incluir também o peso do carro, _wt_, ao quadrado: `formula = mpg ~ hp + wt + I(wt^2)`
- Em `data`, será atribuída uma base de dados e, em `weights`, é possível atribuir ponderações para cada observação (bastante comum em bases providas pelo IBGE, como PNAD e POF)

Vamos regredir o consumo de combustível (_mpg_) pela potência (_hp_) e pelo peso (_wt_) do carro:
```{r}
## Rodar a regressao OLS
fit_ols1 = lm(formula = mpg ~ hp + wt, data = mtcars)

## Resumir os resultados da regressao
summary(fit_ols1)
```

- É possível extrair informações da regressão:
```{r}
names(fit_ols1) # verificar todas informações contidas no objeto da regressão
fit_ols1$coefficients # Estimativas
summary(fit_ols1)$coefficients # Estimativas pelo summary()
head(fit_ols1$fitted.values) # valores ajustados
head(fit_ols1$residuals) # resíduos
```

#### Variáveis categóricas
- Ao inserir **variáveis categóricas** na regressão, criam-se _dummies_ para cada categoria e retira-se uma delas para evitar problema de multicolinearidade:
- No exemplo abaixo, transformaremos a variável de cilindros (_cyl_), que possui apenas 3 valores (4, 6, e 8), na classe _factor_.
```{r}
mtcars_factor1 = mtcars %>% mutate(cyl = factor(cyl))
summary( lm(mpg ~ cyl, data=mtcars_factor1) )$coef
```
- Note que criou apenas 2 _dummies_ de cilindros (_cyl6_ e _cyl8_), sendo que as suas estimativas são negativas e significativas em relação ao carro com 4 cilindros (_cyl4_) que é a caregoria omitida para evitar multicolinearidade.
- Podemos também definir a categoria a ser omitida usando as funções:
  - `factor(..., levels=c(...))`, quando transforma um vetor em factor (omite o 1º)
  - `relevel(..., ref="")`, quando o vetor já é da classe factor e quer apenas alterar os níveis
- Para omitir _cyl8_ fazemos:
```{r}
# Via relevel() - via factor já existente
mtcars_factor1$cyl = relevel(mtcars_factor1$cyl, ref="8")
summary( lm(mpg ~ cyl, data=mtcars_factor1) )$coef

# Via factor() - omite o 1º nível
mtcars_factor2 = mtcars %>% mutate(cyl = factor(cyl, levels = c(8, 4, 6)))
summary( lm(mpg ~ cyl, data=mtcars_factor2) )$coef
```
- Observe que as estimativas estão positivas agora, dado que a referência agora é _cyl8_

#### Exportando output em LaTeX
- Podemos usar os pacotes `stargazer` e `textreg` para gerar o output da regressão em LaTeX
- As funções têm os mesmos nomes dos pacotes:
```r
stargazer::stargazer(fit_ols1)
texreg::texreg(fit_ols1)
```
- Depois, copie o output e jogue num programa de LaTeX.


### Estimação Analítica
- [ResEcon 703](https://github.com/woerman/ResEcon703) - Week 2 (University of Massachusetts Amherst)

#### 1. Construir matrizes de covariadas $X$ e da variável dependente $y$
```{r}
## Criando reg_data a partir de variáveis de mtcars + uma coluna de 1's (p/ constante)
reg_data = mtcars %>% 
  select(mpg, hp, wt) %>%  # Selecionando as variáveis dependente e independentes
  mutate(constante = 1)  # Criando coluna de constante

## Selecionando variáveis independentes no objeto X e convertendo em matriz (n x k)
X = reg_data %>% 
  select(constante, hp, wt) %>%  # Selecionando as covariadas X (incluindo constante)
  as.matrix()  # Transformando em matrix

## Selecionando a variável dependente no objeto y e convertendo em matriz (n x 1)
y = reg_data %>% 
  select(mpg) %>%  # Selecionando as covariadas X (incluindo constante)
  as.matrix()  # Transformando em matrix

## Visualização das primeiras linhas de y e X
head(X)
head(y)
```


#### 2. Estimador $\hat{\beta}$
O estimador de OLS é dado por:
$$ \hat{\beta} = (X'X)^{-1} X' y $$

```{r}
## Estimando os parametros beta
beta_hat = solve(t(X) %*% X) %*% t(X) %*% y # solve() calcula a inversa
beta_hat
```


#### 3. Calcular os valores ajustados $\hat{y}$
$$ \hat{y} = X\hat{\beta} $$
```{r}
## Calculando os valores ajustados de y
y_hat = X %*% beta_hat
colnames(y_hat) = "mpg_hat"
head(y_hat)
```


#### 4. Calcular os resíduos $e$
$$ \varepsilon = y - \hat{y} $$
```{r}
## Calculando os residuos
e = y - y_hat
colnames(e) = "e"
head(e)
```


#### 5. Calcular a variância do termo de erro $s^2$
$$ \hat{\sigma}^2 = \frac{e'e}{n-k} $$
```{r}
## Estimando variancia do termo de erro
sigma2 = t(e) %*% e / (nrow(X) - ncol(X))
sigma2
```


#### 6. Calcular a matriz de covariâncias $\hat{Cov}(\widehat{\beta})$
$$ \widehat{Cov}(\hat{\beta}) = \hat{\sigma}^2 (X'X)^{-1} $$
```{r}
## Estimando a matriz de variancia/covariancia das estimativas beta
vcov_hat = c(sigma2) * solve(t(X) %*% X)
vcov_hat
```


#### 7. Calcular erros padrão, estatísticas t, e p-valores
```{r}
## Calculando erros padrao das estimativas beta
std_err = sqrt(diag(vcov_hat)) # Raiz da diagonal da matriz de covariâncias

## Calculando estatisticas t das estimativas beta
t_stat = beta_hat / std_err

## Calculando p-valores das estimativas beta
p_value = 2 * pt(q = -abs(t_stat), df = nrow(X) - ncol(X)) # 2 x acumulada até estatística t negativa

## Organizando os resultados da regressao em uma matriz
results = cbind(beta_hat, std_err, t_stat, p_value)

## Nomeando as colunas da matriz de resultados
colnames(results) = c('Estimate', 'Std. Error', 't stat', 'Pr(>|t|)')
results
```


### Estimação Numérica

#### 1. Criar função perda que calcula a soma dos desvios quadráticos
- A função para calcular a soma dos desvios quadráticos recebe como inputs:
  - um **vetor** de possíveis valores para $\beta_0$, $\beta_1$ e $\beta_2$
  - uma base de dados
```{r}
desv_quad = function(params, data) {
  # Extraindo os parâmetros para objetos
  beta_0 = params[1]
  beta_1 = params[2]
  beta_2 = params[3]
  
  mpg_ajustado = beta_0 + beta_1*data$hp + beta_2*data$wt # valores ajustados
  desvios = data$mpg - mpg_ajustado # desvios = observados - ajustados
  sum(desvios^2)
}
```


#### 2. Otimização
- Agora encontraremos os parâmetros que minimizam a função perda
$$ \text{argmin}_{\theta \in \Theta} \sum_{i=1}^{N}\left( \text{mpg}_i - \widehat{\text{mpg}}_i \right)^2 $$
<!-- tal que $\Theta = \{ \beta_0, \beta_1, \beta_2 \}$. -->
- Para isto usaremos a função `optim()` que retorna os parâmetros que minimizam uma função (equivalente ao _argmin_):
```yaml
optim(par, fn, gr = NULL, ...,
      method = c("Nelder-Mead", "BFGS", "CG", "L-BFGS-B", "SANN", "Brent"),
      lower = -Inf, upper = Inf,
      control = list(), hessian = FALSE)

par: Initial values for the parameters to be optimized over.
fn: A function to be minimized (or maximized), with first argument the vector of parameters over which minimization is to take place. It should return a scalar result.
method: The method to be used. See ‘Details’. Can be abbreviated.
hessian: Logical. Should a numerically differentiated Hessian matrix be returned?
```
- Colocaremos como input:
  - a função perda criada `desv_quad()`
  - um chute inicial dos parâmetros
    - Note que a estimação pode ser mais ou menos sensível ao valores iniciais, dependendo do método de otimização utilizado
    - O mais comum é encontrar como chute inicial um vetor de zeros `c(0, 0, 0)`, por ser mais neutro em relação ao sinal das estimativas
    - Em Econometria III, prof. Laurini recomendou usar método "Nelder-Mead" (padrão) com um chute inicial de zeros e, depois, usar suas estimativas como chute inicial para o método "BFGS".
  - Por padrão, temos o argumento `hessian = FALSE`, coloque `TRUE` para calcularmos o erro padrão, estatística t e p-valor das estimativas

```{r}
# Estimação por BFGS
theta_ini = c(0, 0, 0) # Chute inicial de beta_0, beta_1 e beta_2

fit_ols2 = optim(par=theta_ini, fn=desv_quad, data=mtcars,
                  method="BFGS", hessian=TRUE)
fit_ols2
```


<!-- ```{r} -->
<!-- ## Calculando os erros padrão MLE -->
<!-- fit_ols2_se = fit_ols2$hessian %>%  -->
<!--   solve() %>%  -->
<!--   diag() %>%  -->
<!--   sqrt() -->

<!-- ## Calculando as estatisticas z MLE -->
<!-- fit_ols2_zstat = fit_ols2$par / fit_ols2_se -->

<!-- ## Calculando os p-valores MLE -->
<!-- fit_ols2_pvalue = 2 * pnorm(q = -abs(fit_ols2_zstat)) -->


<!-- ## Montando tabela resumo -->
<!-- summary_tab = matrix(0, length(fit_ols2$par), 4)  # criando matriz de zeros de dimensao 3 x 4 -->
<!-- rownames(summary_tab) = c("beta_0", "beta_hp", "beta_wt") -->
<!-- colnames(summary_tab) = c("Estimate", "Std. Error", "z value", "Pr(>|z|)") -->

<!-- summary_tab[,1] = fit_ols2$par  # 1a coluna com coeficiente -->
<!-- summary_tab[,2] = fit_ols2_se  # 2a coluna com erros padrao -->
<!-- summary_tab[,3] = fit_ols2_zstat  # 3a coluna com estat. z -->
<!-- summary_tab[,4] = fit_ols2_pvalue  # 4a coluna com p-valor -->

<!-- round(summary_tab, 6) -->
<!-- ``` -->

<!-- - Cálculo do erro padrão também poderia ser feito por bootstrap -->
<!-- ```{r} -->
<!-- reps = 100 -->
<!-- est_tab = matrix(0, reps, 3) -->

<!-- for (i in 1:reps) { -->
<!--   i_resample = sample(1:nrow(mtcars), nrow(mtcars), replace=TRUE) -->
<!--   resample = mtcars[i_resample, c("mpg", "hp", "wt")] -->

<!--   theta_ini = c(0, 0, 0) -->
<!--   fit_resample = optim(par=theta_ini, fn=desv_quad, data=resample, -->
<!--                        method="BFGS") -->

<!--   est_tab[i,] = fit_resample$par -->
<!-- } -->

<!-- head(est_tab, 10) -->
<!-- apply(est_tab, 2, sd) -->
<!-- ``` -->

## Métodos de Otimização Básicos
- Essa seção tem o objetivo para dar uma intuição sobre métodos de otimização e está baseada em Hamilton (1994), seção 5.7.
- Veremos os métodos de _grid search_ (discretização) e _steepest ascent_ que são métodos simples, mas que representam famílias de métodos de otimização.
- Para maior profundidade no assunto, sugiro cursar disciplina de Economia Computacional.

### _Grid Search_
```{r, echo=FALSE}
# Define variable containing url
url = "https://fhnishida.github.io/fearp/eco1/grid_search.png"
```

- O método mais simples de otimização numérica é o _grid search_ (discretização).
- Como o R não lida com problemas com infinitos valores, uma forma lidar com isso é discretizando diversos possíveis valores dos parâmetros de escolha dentro de intervalos.
- Para cada possível combinação de parâmetros, calculam-se diversos valores a partir da função objetivo. De todos os valores calculados, escolhe-se a combinação de parâmetros que maximizam (ou minimizam) a função objetivo.
- O exemplo abaixo considera apenas um parâmetro de escolha $\theta$ e, para cada ponto escolhido dentro do intervalo $[-1, 1]$, calcula-se a função objetivo:

<center><img src="`r url`"></center>

- Este é um método robusto a funções com descontinuidades e quinas (não diferenciáveis), e menos sensível a chutes de valores iniciais. (ver método abaixo)
- Porém, por ter que fazer cálculo da função objetivo para inúmeros pontos, tende a ser menos eficiente.


### _Steepest Ascent_

```{r, echo=FALSE}
# Define variable containing url
url = "https://fhnishida.github.io/fearp/eco1/steepest_ascent.png"
```

- Conforme o número de parâmetros do modelo cresce, aumenta o número de possíveis combinações entre parâmetros e torna o processo computacional cada vez mais lento.
- Uma forma mais eficiente de encontrar o conjunto de parâmetros que otimizam a função objetivo é por meio do método _steepest ascent_.
- Seja $\theta^*$ o conjunto de parâmetros que maximiza a função objetivo:
  1. Comece com alguns valores iniciais dos parâmetros, $\theta^0$
  2. Calcula-se o gradiente e avalia-se a possibilidade de "andar para cima" a um valor mais alto
  3. Caso possa, ande na direção correta a $\theta_1$
  4. Repita os passos (2) e (3), andando de $\theta^s$ para $\theta^{s+1}$ até
  atingir o máximo com $\theta^*$.

<center><img src="`r url`"></center>

- Note que esse método de otimização é sensível ao conjunto de parâmetros iniciais e a descontinuidades da função objetivo.
- Por outro lado, é um método mais eficiente (calcula uma função objetivo dado os parâmetros a cada passo que dá) e tende a ser também mais preciso nas estimações.



## Estimação por MLE
- [ResEcon 703](https://github.com/woerman/ResEcon703) - Week 6 (University of Massachusetts Amherst)

<!-- Para uma equação de regressão geral -->
<!-- $$ y_i = \beta' X + \varepsilon $$ -->
<!-- supondo distribuição normal do termo de erro -->
<!-- $$ \varepsilon \sim \mathcal{N}(0, \sigma^2), $$ -->
<!-- temos uma distribuição condicional de $y$ dada por -->
<!-- $$ y | X \sim \mathcal{N}(\beta'X, \sigma^2). $$ -->

<!-- Logo, a função log-verossimilhança (condicional) é -->
<!-- $$ \ln{L(\beta, \sigma^2 | y, X)} = \sum^n_{i=1}{\ln{f(y | X, \beta, \sigma^2)}}. $$ -->

<!-- Em nosso exemplo, temos que estimar 4 parâmetros -->
<!-- $$ \theta = \left( \beta_0, \beta_1, \beta_2, \sigma^2 \right). $$ -->

<!-- Podemos: -->

<!-- - Tomar derivadas de $\ln{L(\beta, \sigma^2 | y, X)}$ em relação a cada parâmetro e resolver as CPOs, ou -->
<!-- - Maximizar $\ln{L(\beta, \sigma^2 | y, X)}$ por otimização numérica. -->

### Intuição do cálculo da função de verossimilhança
- Apenas para ilustrar a construção da função de verossimilhança, considere um modelo logit em que queremos estimar os indivíduos precisam escolher se usam carro ou ônibus para deslocamento.
- Para estimar as probabilidades de usar carro (e, por consequência, de ônibus), vamos utilizar as informações dos preços que os indivíduos pagam pela gasolina e pela passagem de ônibus.
- Note que os valores abaixo foram todos inventados.
- Considere um conjunto de parâmetros $\theta^A = \{ \beta^A_0, \beta^A_1, \beta^A_2 \}$ que gerem as seguintes probabilidades de usar carro e de ônibus (últimas 2 colunas):

|             | **Preço Gasolina** | **Preço Bus** | **Escolha** | **_Prob(Car  $| \theta^A$)_** | **_Prob(Bus $| \theta^A$)_** |
|:-----------:|:------------------:|:----------------:|:-----------:|:-----------------:|:------------------:|
| Indiv. 1 |        6,93        |       5,00       |    Car    |        **0,63**       |        0,37        |
| Indiv. 2 |        7,01        |       2,50       |    Bus   |        0,33       |        **0,67**        |
| Indiv. 3 |        6,80        |       2,50       |    Bus   |        0,25       |        **0,75**        |
| Indiv. 4 |        6,75        |       5,00       |    Car    |        **0,73**       |        0,27        |

- Logo, a verossimilhança, dado os parâmetros $\theta^A$ é
$$ \mathcal{L}(\theta^A) = 0,63 \times 0,67 \times 0,75 \times 0,73 = 0,231 $$
- Agora, considere $\theta^B = \{ \beta^B_0, \beta^B_1, \beta^B_2 \}$ que gerem as seguintes probabilidades:

|             | **Preço Gasolina** | **Preço Bus** | **Escolha** | **_Prob(Car  $| \theta^B$)_** | **_Prob(Bus $| \theta^B$)_** |
|:-----------:|:------------------:|:----------------:|:-----------:|:-----------------:|:------------------:|
| Indiv. 1 |        6,93        |       5,00       |    Car    |        **0,54**       |        0,46        |
| Indiv. 2 |        7,01        |       2,50       |    Bus   |        0,43       |        **0,57**        |
| Indiv. 3 |        6,80        |       2,50       |    Bus   |        0,35       |        **0,65**        |
| Indiv. 4 |        6,75        |       5,00       |    Car    |        **0,58**       |        0,42        |

- Então, a verossimilhança, dado $\theta^B$, é
$$ \mathcal{L}(\theta^B) = 0,54 \times 0,57 \times 0,65 \times 0,58 = 0,116 $$
- Como $\mathcal{L}(\theta^A) = 0,231 > 0,116 = \mathcal{L}(\theta^B)$, então os parâmetros $\theta^A$ se mostram mais adequados em relação a $\theta^B$
- Na máxima verossimilhança (MLE), é escolhido o conjunto de parâmetros $\theta^*$ que maximiza a função de verossimilhança (ou log-verossimilhança).
- No modelo logit, as probabilidades usadas para calcular a verossimilhança são as próprias proabilidades de escolha por uma alternativa, dado um conjunto de parâmetros.
- Já no modelo linear, usamos a função de densidade de probabilidade para avaliar a "distância" de cada observação, $y_i$, em relação ao seu valor ajustado $\hat y_i$, dado um conjunto de parâmetros.



### Otimização Numérica para MLE
A função `optim()` do R será usada novamente para desempenhar a otimização numérica. Precisamos usar como input:

- Alguns valores inicias dos parâmetros, $\theta^0$
- Uma função que tome esses parâmetros como um argumento e calcule a 
log-verossimilhança, $\ln{L(\theta)}$.

<!-- Como `optim()` irá encontrar os parâmetros que minimizem a função objetivo, precisamos adaptar o output da função de log-verossimilhança (minimizaremos o negativo da log-lik). -->

A função log-verossimilhança é dada por
$$ \ln{L(\beta, \sigma^2 | y, X)} = \sum^n_{i=1}{\ln{f(y_i | x_i, \beta, \sigma^2)}}, $$
em que a distribuição condicional de cada $y_i$ é
$$ y_i | x_i \sim \mathcal{N}(\beta'x_i, \sigma^2) $$

1. Construir matriz $X$ e vetor $y$
2. Calcular os valores ajustados de $y$, $\hat{y} - \beta'x_i$, que é a média de cada $y_i$
3. Calcular a densidade para cada $y_i$, $f(y_i | x_i, \beta, \sigma^2)$
4. Calcular a log-verossimilhança, $\ln{L(\beta, \sigma^2 | y, X)} = \sum^n_{i=1}{\ln{f(y_i | x_i, \beta, \sigma^2)}}$


#### 1. Chute de valores iniciais para $\beta_0, \beta_1, \beta_2$ e $\sigma^2$
- Note que, diferente da estimação por OLS, um dos parâmetros a ser estimado via MLE é a variância ($\sigma^2$).
```{r}
params = c(35, -0.02, -3.5, 1)
# (beta_0, beta_1 , beta_2, sigma2)
```

#### 2. Seleção da base de dados e variáveis
```{r}
## Adicionando colunas de 1's para o termo constante
data = mtcars
beta_0 = params[1]
beta_1 = params[2]
beta_2 = params[3]
sigma2 = params[4]
```

#### 3. Cálculo dos valores ajustados e das densidades
```{r}
## Calculando valores ajustados de y
y_hat = beta_0 + beta_1*data$hp + beta_2*data$wt
```

#### 4. Cálculo das densidades
$$ f(y_i | x_i, \beta, \sigma^2) $$
```{r}
## Calculando os pdf's de cada outcome
y_pdf = dnorm(data$mpg, mean = y_hat, sd = sqrt(sigma2))

head(y_pdf) # Primeiros valores da densidade
prod(y_pdf) # Verossimilhança
```

- Para entender melhor o que estamos fazendo aqui, relembre que, na estimação por máxima verossimilhança, assume-se que
$$\varepsilon | X \sim N(0, \sigma^2)$$
- No exemplo abaixo, podemos ver que, para cada $x$, temos um valor ajustado $\hat{y} = \beta_0 + \beta_1 x$ e seus desvios $\varepsilon$ são normalmente distribuídos com a mesma variância $\sigma^2$
```{r, echo=FALSE}
# Define variable containing url
url = "https://fhnishida.github.io/fearp/eco1/mle.jpg"
```
<center><img src="`r url`"></center>
- Agora, vamos juntar o data frame `mtcars` com os valores ajustados `mpg_hat` e as densidades `y_pdf`:
```{r}
mpg_hat = y_hat # atribuindo y_hat a um objeto com nome mais adequado

# Juntando as bases e visualizando os primeiros valores
bd_joined = cbind(mtcars, mpg_hat, y_pdf) %>%
  select(hp, wt, mpg, mpg_hat, y_pdf)
head(bd_joined)
```
- Como pode ser visto na base de dados juntada e nos gráficos abaixo, quanto mais próximo o valor ajustado for do valor observado de cada observação, maior será a densidade/probabilidade.
```{r}
# Criando gráfico para os 2 primeiros carros (Mazda RX4 e Mazda RX 4 Wag)
qt_norm = seq(20, 27, by=0.1) # valores de mpg ("escores Z")

# Mazda RX4
pdf_norm1 = dnorm(qt_norm, mean=bd_joined$mpg_hat[1], sd=sqrt(sigma2)) # pdf
plot(qt_norm, pdf_norm1, type="l", xlab="mpg", ylab="densidade", main="Mazda RX4")
abline(v=c(bd_joined$mpg_hat[1], bd_joined$mpg[1]), col="red")
text(c(bd_joined$mpg_hat[1], bd_joined$mpg[1]), 0.2, 
     c(expression(widehat(mpg)[1]), expression(mpg[1])), 
     pos=2, srt=90, col="red")

# Mazda RX4 Wag 
pdf_norm2 = dnorm(qt_norm, mean=bd_joined$mpg_hat[2], sd=sqrt(sigma2)) # pdf
plot(qt_norm, pdf_norm2, type="l", xlab="mpg", ylab="densidade", main="Mazda RX4 Wag")
abline(v=c(bd_joined$mpg_hat[2], bd_joined$mpg[2]), col="blue")
text(c(bd_joined$mpg_hat[2], bd_joined$mpg[2]), 0.2, 
     c(expression(widehat(mpg)[2]), expression(mpg[2])), 
     pos=2, srt=90, col="blue")
```
- Logo, a verossimilhança (produto de todas probabilidades) será maior quanto mais próximos forem os valores ajustados dos seus respectivos valores observados.


#### 5. Calculando a Log-Verossimilhança
$$ \mathcal{l}(\beta, \sigma^2) = \sum^{N}_{i=1}{\ln\left[ f(y_i | x_i, \beta, \sigma^2) \right]} $$
```{r}
## Calculando a log-verossimilhanca
loglik = sum(log(y_pdf))
loglik
```


#### 6. Criando a Função de Log-Verossimilhança
```{r}
## Criando funcao para calcular log-verossimilhanca OLS 
loglik_lm = function(params, data) {
  # Pegando os parâmetros
  beta_0 = params[1]
  beta_1 = params[2]
  beta_2 = params[3]
  sigma2 = params[4]
  
  ## Calculando valores ajustados de y
  y_hat = beta_0 + beta_1*data$hp + beta_2*data$wt
  
  ## Calculando os pdf's de cada outcome
  y_pdf = dnorm(data$mpg, mean = y_hat, sd = sqrt(sigma2))
  
  ## Calculando a log-verossimilhanca
  loglik = sum(log(y_pdf))
  
  ## Retornando o negativo da log-verossimilanca - optim() não maximiza
  -loglik
}
```


#### 7. Otimização

Tendo a função objetivo, usaremos `optim()` para *minimizar*
$$ -\ln{L(\beta, \sigma^2 | y, X)} = -\sum^n_{i=1}{\ln{f(y_i | x_i, \beta, \sigma^2)}}. $$
Aqui, **minimizamos o negativo** da log-Verossimilhança para **maximizarmos** (função`optim()` apenas minimiza).

```{r warning=FALSE}
## Maximizando a função log-verossimilhança OLS
mle = optim(par = c(0, 0, 0, 1), fn = loglik_lm, data = mtcars,
              method = "BFGS", hessian = TRUE)

## Mostrando os resultados da otimização
mle

## Calculando os erros padrão
mle_se = mle$hessian %>% # hessiano
  solve() %>% # toma a inversa para obter a matriz de var/cov
  diag() %>% # pega a diagonal da matriz
  sqrt() # calcula a raiz quadrada

# Visualizando as estimativas e os erros padrão
cbind(mle$par, mle_se)
```


<!-- #### Cálculo dos erros padrão via Bootstrap -->

<!-- ```{r warning=FALSE} -->
<!-- reps = 50 -->
<!-- est_tab = matrix(0, reps, 4) -->

<!-- for (i in 1:reps) { -->
<!--   i_resample = sample(1:nrow(mtcars), nrow(mtcars), replace=TRUE) # reamostragem de índices c/ reposição -->
<!--   resample = mtcars[i_resample, c("mpg", "hp", "wt")] -->

<!--   fit_resample = optim(par = mle$par, fn = loglik_lm, data = resample, -->
<!--                        method = "BFGS", hessian = TRUE) -->

<!--   est_tab[i,] = fit_resample$par -->
<!-- } -->

<!-- head(est_tab, 10) # estimativas bootstrapeadas -->
<!-- apply(est_tab, 2, sd) # erros padrão bootstrapeados -->
<!-- ``` -->



## Estimação por GMM
- [Computing Generalized Method of Moments and Generalized Empirical Likelihood with R (Pierre Chaussé)](https://cran.r-project.org/web/packages/gmm/vignettes/gmm_with_R.pdf)
- [Generalized Method of Moments (GMM) in R - Part 1 (Alfred F. SAM)](https://medium.com/codex/generalized-method-of-moments-gmm-in-r-part-1-of-3-c65f41b6199)


- Para estimar via GMM precisamos construir vetores relacionados aos seguintes momentos:
$$ E(\varepsilon) = 0 \qquad \text{ e } \qquad E(\varepsilon'X) = 0 $$
em que $X$ é a matriz de covariadas e $\varepsilon$ é o desvio. Note que estes são os momentos relacionados ao OLS, dado que este é um caso particular do GMM.


- Relembre que estamos usando a base de dados `mtcars` para estimar o modelo linear:
$$ \text{mpg} = \beta_0 + \beta_1 \text{hp} + \beta_2 \text{wt} + \varepsilon $$
que relaciona o consumo de combustível (em milhas por galão - _mpg_) com a potência (_hp_) e o peso (em mil libras - _wt_) do carro.


### Otimização Numérica para GMM

#### 1. Chute de valores iniciais para $\beta_0$, $\beta_1$ e $\beta_2$
- Vamos criar um vetor com possíveis valores de $\beta_0, \beta_1, \beta_2$:
```{r}
library(dplyr)

params = c(35, -0.02, -3.5)
beta_0 = params[1]
beta_1 = params[2]
beta_2 = params[3]
```

#### 2. Seleção da base de dados e variáveis
```{r}
data = mtcars %>% mutate(constant=1) # Criando variável de constante

## Selecionando colunas para X (covariadas) e convertendo para matrix
X = data %>% 
  select("constant", "hp", "wt") %>% 
  as.matrix()

## Selecionando variavel para y e convertendo para matrix
y = data %>% 
  select("mpg") %>% 
  as.matrix()
```

#### 3. Cálculo dos valores ajustados e dos desvios
```{r}
## Valores ajustados e desvios
y_hat = X %*% params
# equivalente a: y_hat = beta_0 + beta_1 * X[,"hp"] + beta_2 * X[,"wt"]

e = y - y_hat
```

#### 4. Criação da matriz de momentos
- Note que $E(\varepsilon' X)$ é uma multiplicação matricial, mas a função `gmm()` exige que como input os vetores com multiplicação elemento a elemento do resíduo $\varepsilon$ com as covariadas $X$ (neste caso: constante, hp, wt)
```{r}
m = X * as.vector(e) # matriz de momentos (sem tomar esperança)
head(m)
```
- Note que, como multiplicamos a constante igual a 1 com os desvios $\varepsilon$, a 1ª coluna corresponde ao momento $E(\varepsilon)=0$ (mas sem tomar a esperança).
- Já as colunas 2 e 3 correspodem ao momento $E(\varepsilon'X)=0$ para as variáveis _hp_ e _wt_ (também sem tomar a esperança).
- Logicamente, para estimar por GMM, precisamos escolher os parâmetros $\theta = \{ \beta_0, \beta_1, \beta_2 \}$ que, ao tomar a esperança em cada um destas colunas, se aproximem ao máximo de zero. Isso será feito via função `gmm()` (semelhante à função `optim()`)


#### 5. Criação de função com os momentos
- Vamos criar uma função que tem como input um vetor de parâmetros (`params`) e uma base de dados (`data`), e que retorna uma matriz em que cada coluna representa um momento.
- Essa função incluirá todos os comandos descritos nos itens 1 a 4 (que, na verdade, apenas foram feitos por didática).
```{r}
mom_ols = function(params, data) {
  ## Adicionando colunas de 1's para o termo constante
  data = data %>% mutate(constant = 1)
  
  ## Selecionando colunas para X (covariadas) e convertendo para matrix
  X = data %>% 
    select("constant", "hp", "wt") %>% 
    as.matrix()
  
  ## Selecionando variavel para y e convertendo para matrix
  y = data %>% 
    select("mpg") %>% 
    as.matrix()
  
  ## Valores ajustados e desvios
  y_hat = X %*% params
  e = y - y_hat
  
  m = as.vector(e) * X # matriz de momentos (vetor - multiplicação por elemento)
  m
}
```


#### 6. Otimização via função `gmm()`
- A função `gmm()`, assim como a `optim()`, recebe uma função como argumento.
- No entanto, ao invés de retornar um valor, a função que entra no `gmm()` retorna uma matriz, cujas médias das colunas queremos aproximar de zero. 
```{r}
library(gmm)

gmm_lm = gmm(mom_ols, mtcars, c(0,0,0),
             wmatrix = "optimal", # matriz de ponderação
             optfct = "nlminb" # função de otimização
             )

summary(gmm_lm)$coefficients
```


# Dados em Painel

## Manipulação de dados em painel
- Para o que estamos estudando, é normalmente exigido que os dados estejam
    - no formato _long_: para cada indivíduo, temos uma linha para cada período;
    - _balanceados_: o tamanho da amostra é $N \times T$, com $N$ indivíduos e $T$ períodos; e
    - devidamente ordenados por indivíduos e, depois, por tempo.

```{r, echo=FALSE}
# Define variable containing url
url = "https://www.theanalysisfactor.com/wp-content/uploads/2013/10/image002.jpg"
```
<center><img src="`r url`"></center>

- Em muitos casos, as informações são disponibilizadas em várias bases de dados de cortes transversais (_cross sections_), então é necessário estruturar a base de dados em painel.
- Isso por ser feito no R de, pelo menos, duas formas:
    - empilhando as bases de dados e filtrando apenas indivíduos que aparecem em todos períodos; ou
    - fazendo a junção interna (_inner join_) das bases por indivíduo e transformando do formato _wide_ para o _long_.
- Como exemplo, usaremos a PNAD Contínua que é publicada trimestralmente e possui o pacote `PNADcIBGE` que auxilia na sua utilização.
- Os dados podem ser obtidos via`read_pnadc(microdata, input_txt)` que necessita que você faça download das bases de dados e do txt com informações das variáveis (_input_txt_) no [FTP do IBGE](https://ftp.ibge.gov.br/Trabalho_e_Rendimento/Pesquisa_Nacional_por_Amostra_de_Domicilios_continua/Trimestral/Microdados/):
```{r}
# install.packages("PNADcIBGE")
library(PNADcIBGE)
library(dplyr)
```
- O arquivo compactado .zip é cerca de 12\% do arquivo descompactado .txt (133mb $\times$ 1,08gb). Para não precisar manter o arquivo .txt no computador, podemos usar a função `unz()` para descompactar arquivos temporariamente:
```{r}
# Descompactando as bases da PNADc e carregando no R
pnad_012021 = read_pnadc(unz("PNADc/PNADC_012021_20220224.zip", "PNADC_012021.txt"),
                         input_txt = "PNADc/input_PNADC_trimestral.txt")

pnad_022021 = read_pnadc(unz("PNADc/PNADC_022021_20220224.zip", "PNADC_022021.txt"),
                         input_txt = "PNADc/input_PNADC_trimestral.txt")
```
- Ou também via `get_pnadc(year, quarter = NULL, design = TRUE)`, que faz o download diretamente do R e atribui para um objeto. É necessário informar a data da pesquisa (ano e trimestre) e, para retornar um data frame, altere o argumento para `design = FALSE` (caso contrário, irá retornar um objeto do tipo `survey.design`). Além disso, constrói automaticamente colunas com deflatores:
```r
# OU Carregando as bases da PNADc via get_pnadc()
pnad_012021 = get_pnadc(year=2021, quarter=1, design=FALSE)
pnad_022021 = get_pnadc(year=2021, quarter=2, design=FALSE)

```

- Para identificar um indivíduo na base do PNAD, o IBGE usa as seguintes [variáveis-chave](https://www.ibge.gov.br/estatisticas/downloads-estatisticas.html?caminho=Trabalho_e_Rendimento/Pesquisa_Nacional_por_Amostra_de_Domicilios_continua/Trimestral/Microdados/Documentacao):
    - _UPA_: Unidade Primária de Amostragem / UF (2) + Nº Sequencial (6) + DV (1)
    - _V1008_: Número do domicílio (01 a 14)
    - _V1014_: Painel/Grupo de amostra (01 a 99)
    - _V2003_: Número de ordem (01 a 30)
- Pesquisadores do Ipea ([Teixeira Júnior et al., 2020](http://repositorio.ipea.gov.br/bitstream/11058/9951/1/bmt_67_nt_pesos_longitudinais.pdf)) usam mais algumas variáveis-chave invariantes no tempo para tornar esse 
    - _V2007_: Sexo
    - _V2008_/_V20081_/_V20082_: Data de nascimento (dia/mês/ano)
- Além disso, vamos adicionar mais algumas variáveis:
    - _invariante no tempo_:
        - _UF_: Unidade da Federação
    - _variantes no tempo_:
        - _V2009_: Idade (em anos)
        - _VD4020_: Rendimento mensal efetivo de todos os trabalhos para pessoas de 14 anos ou mais de idade

```{r}
# Boas práticas (pricipalmente usando base de dados grandes):
# - Não manipular o objeto em que você carregou a base de dados -> crie um novo
# - Selecione apenas as variáveis que for utilizar

lista_var = c("Trimestre", "UPA", "V1008", "V1014", "V2003", "V2007", "V2008",
              "V20081", "V20082", "UF", "V2009", "VD4020")

# Selecionando e renomeando variáveis, e filtrando apenas maiores de 14 anos 
pnad_1 = pnad_012021 %>% select(all_of(lista_var)) %>%
    rename(DOMIC = V1008, PAINEL = V1014, ORDEM = V2003, SEXO = V2007, 
           DIA_NASC = V2008, MES_NASC = V20081, ANO_NASC = V20082, 
           IDADE = V2009, RENDA = VD4020) %>%
    filter(IDADE >= 14)

pnad_2 = pnad_022021 %>% select(all_of(lista_var)) %>%
    rename(DOMIC = V1008, PAINEL = V1014, ORDEM = V2003, SEXO = V2007, 
           DIA_NASC = V2008, MES_NASC = V20081, ANO_NASC = V20082, 
           IDADE = V2009, RENDA = VD4020) %>%
    filter(IDADE >= 14)
```



### Empilhando bases de dados e filtrando indivíduos que aparecem em todos os períodos
- Primeiro, empilharemos as bases de dados usando `rbind()`. É necessário garantir que tenham o mesmo número de colunas e estas sejam da mesma classe (_character_, _numeric_, etc.):
```{r}
pnad_bind = rbind(pnad_1, pnad_2)
head(pnad_bind)
```
- Note que a 2ª observação não corresponde à mesma pessoa da 1º linha. Vamos criar uma variável `ID`, juntando informações de todas variáveis-chave, e rearranjar a base de dados de acordo com ela e o trimestre:
```{r}
pnad_bind = pnad_bind %>% mutate(
    ID = paste0(UPA, DOMIC, PAINEL, ORDEM, SEXO, DIA_NASC, MES_NASC, ANO_NASC)
    ) %>% select(ID, everything()) %>% # reordenando variáveis, começando com ID
    arrange(ID, Trimestre)
head(pnad_bind, 10)
```
- Observe que o base de dados em painel não está balanceada, ou seja, nem todos os indivíduos aparecem nos 2 trimestres. Portanto, vamos criar um objeto auxiliar com a contagem de vezes que o `ID` aparece em `pnad_bind`
```{r}
cont_ID = pnad_bind %>% group_by(ID) %>% summarise(cont = n())
head(cont_ID, 10)
```
- Em `cont_ID`, vamos filtrar apenas os caso que aparecem 2 vezes
```{r}
cont_ID = cont_ID %>% filter(cont == 2)
head(cont_ID, 10)
```
- Voltando para a base `pnad_bind`, vamos filtrar apenas ID's que aparecem no vetor `cont_ID$ID`:
```{r}
pnad_bind = pnad_bind %>% filter(ID %in% cont_ID$ID)
head(pnad_bind)
N = pnad_bind$ID %>% unique() %>% length() # Nº de indivíduos únicos
T = pnad_bind$Trimestre %>% unique() %>% length() # Nº de trimestre únicos
paste0("N = ", N, ", T = ", T, ", NT = ", N*T)
```


### Juntado as bases e transformando de _wide_ para _long_
- Agora, juntaremos a base usando a função `inner_join()` que apenas mantém indivíduos que aparecem em ambas bases de dados:
```{r}
pnad_joined = inner_join(pnad_1, pnad_2, 
                         by=c("UPA", "DOMIC", "PAINEL", "ORDEM", "SEXO",
                              "DIA_NASC", "MES_NASC", "ANO_NASC"),
                         suffix=c("_1", "_2")) # evite usar . como separador
colnames(pnad_joined) # nomes das colunas
dim(pnad_joined) # dimensões da base de dados
```
- Note que obtivemos a base no formato _wide_ (1 linha para cada indivíduo) e as informações relativas aos 2 períodos (1º e 2º trimestres de 2021) estão em colunas:
    - Os sufixos foram utilizamos para duplicar colunas de informações contidas em ambas bases (e que não foram inseridas no argumento `by`).
    - A variável invariante no tempo _UF_ foi duplicada, então seria interessante incluí-la também como uma ``variável-chave''
```{r}
pnad_joined = inner_join(pnad_1, pnad_2, 
                         by=c("UPA", "DOMIC", "PAINEL", "ORDEM", "SEXO",
                              "DIA_NASC", "MES_NASC", "ANO_NASC", "UF"),
                         suffix=c("_1", "_2")) # evite usar . como separador
colnames(pnad_joined) # nomes das colunas
dim(pnad_joined) # dimensões da base de dados
```
- Observe que temos uma única coluna _UF_ agora e o número de observações manteve-se inalterado, pois os domicílios da amostra de fato não alteraram suas UFs entre estes trimestres.
    - Caso alterasse o número de linhas, a variável invariante no tempo possui algumas observações que alteraram entre os períodos e estas foram excluídas da amostra.
- Também podemos retirar as colunas "Trimestre.1" e "Trimestre.2":
```{r}
pnad_joined = pnad_joined %>% select(-Trimestre_1, -Trimestre_2)
```
- Estando no formato _wide_, precisamos transformar para o formato _long_

#### Transformando a base de _wide_ para _long_ via `tidyr`
- [Pivoting (_tidyr_)](https://tidyr.tidyverse.org/articles/pivot.html)
<!-- - As bases de dados costumam ser disponibilizadas no formato _wide_, em que cada linha corresponde a uma observação/indivíduo. Já o formato _long_, refere-se ao formato em que cada observação/indivíduo aparece em mais de uma linha: -->
<!--     - em que cada indivíduo é observado mais de uma vez no tempo (dados em painel ou  séries temporais), ou -->
<!--     - em que cada indivíduo toma decisão considerando várias possíveis escolhas (logit multinomial) -->
<!-- - Logo, recorrentemente, precisamos transformar bases do formato _wide_ no formato _long_, ou vice-versa. -->
- Para fazer transformações em _wide_ ou _long_ usaremos o pacote `tidyr` e suas funções `pivot_longer()`, `pivot_wider()` e `separate()`
```{r}
library(tidyr)
```
- `pivot_longer()`: transforma várias colunas em duas: de nomes e de valores (aumenta o nº de linhas e diminui o de colunas)
```yaml
pivot_longer(
  data,
  cols,
  names_to = "name",
  values_to = "value"
  ...
)
```
- `pivot_wider()`: transforma nomes (valores únicos) de uma variável em várias colunas (aumenta o nº de colunas e diminui o de linhas)
```yaml
pivot_wider(
  data,
  names_from = name,
  values_from = value,
  values_fill = NULL
  ...
)
```
- `separate()`: divide uma coluna em outras a partir de um caracter ``separador''
```yaml
separate(
  data,
  col,
  into,
  sep = "[^[:alnum:]]+"
  ...
)
```

- Primeiro, vamos transformar as colunas variantes no tempo (com sufixos _1 ou _2) em duas colunas
```{r}
library(tidyr)
pnad_joined2 = pnad_joined %>%
    pivot_longer(
        cols = c(ends_with("_1"), ends_with("_2") ),
        names_to = "VAR_TRI", # nome da coluna que vão os nomes das colunas antigas
        values_to = "VALUE" # nome da coluna com os valores das colunas transformadas
    )
head(pnad_joined2)
```
- Note que, ao invés de ter 2 linhas por indivíduo, temos 4 (pois temos 2 variáveis variantes no tempo).
- Precisamos jogar metade das linhas de volta para colunas. Vamos usar a função `separate()` para separar _VAR.TRI_ (com 4 valores únicos: _IDADE_1_, _IDADE_2_, _RENDA_1_ e _RENDA_2_) em 2 colunas: _VAR_ (2 valores únicos: _IDADE_ e _RENDA_) e _TRI_ (2 valores únicos: _1_ e _2_).
```{r}
pnad_joined3 = pnad_joined2[1:100,] %>%
    separate(
        col = "VAR_TRI",
        into = c("VAR", "TRI"), # nomes das colunas separadas
        sep = "_" # caracter que separa as valores da coluna VAR_TRI
    )
head(pnad_joined3)
```

- Para finalizar, vamos transformar a coluna _VAR_ (com 2 valores únicos: _IDADE_ e _RENDA_) em 2 colunas (_IDADE_ e _RENDA_):
```{r}
pnad_joined4 = pnad_joined3 %>%
    pivot_wider(
        names_from = "VAR",
        values_from = "VALUE"
    )
pnad_joined4 %>% select(TRI, everything()) %>% head(20)
```


#### Extra: Criação de dummies via `pivot_wider()`
- Primeiro, é necessário criar uma coluna de 1's
- Depois usar a função `pivot_wider()`, indicando a variável categórica e a coluna de 1's, preenchendo os NA's com zero (`fill = 0`) :
```{r}
dummies_sexo = pnad_1 %>% mutate(const = 1) %>% # criando coluna de 1's
    pivot_wider(names_from = SEXO,
                values_from = const,
                values_fill = 0)
head(dummies_sexo)
```


#### Outro exemplo 1: _wide_ $\rightarrow$ _long_
- A base de dados abaixo possui informações de 5 condados com suas repectivas áreas territoriais, proporções de adultos com ensino superior e nº de vagas de emprego em 4 anos distintos:
```{r}
bd_counties = data.frame(
    county = c("Autauga", "Baldwin", "Barbour", "Bibb", "Blount"),
    area = c(599, 1578, 891, 625, 639),
    college_1970 = c(.064, .065, .073, .042, .027),
    college_1980 = c(.121, .121, .092, .049, .053),
    college_1990 = c(.145, .168, .118, .047, .070),
    college_2000 = c(.180, .231, .109, .071, .096),
    jobs_1970 = c(6853, 19749, 9448, 3965, 7587),
    jobs_1980 = c(11278, 27861, 9755, 4276, 9490),
    jobs_1990 = c(11471, 40809, 12163, 5564, 11811),
    jobs_2000 = c(16289, 70247, 15197, 6098, 16503)
)
bd_counties
```
- Queremos estruturar a base de dados de modo que, para cada condado, tenhamos 4 linhas (cada uma corresponde a um dos anos: 1970, 1980, 1990 ou 2020). Portanto, teremos 5 colunas: _county_, _year_, _area_, _college_ e _jobs_. Começamos transformando as colunas cujos nomes iniciam com `college_` e com `jobs_` em linhas via `pivot_longer()`:
```{r}
bd_counties2 = bd_counties %>%
    pivot_longer(
        cols = c( starts_with("college_"), starts_with("jobs_") ),
        names_to = "var_year", # nome da coluna que vão os nomes das colunas antigas
        values_to = "value" # nome da coluna com os valores das colunas transformadas
    )
head(bd_counties2, 10)
```
- Note que, para cada condado, há duas linhas para cada ano, já que há 2 que variam no tempo (_college_ e _jobs_). Precisamos tirar essa duplicidade de anos. Começamos usando a função `separate()` para separar a variável `var_year` em duas colunas (que chamaremos de `var` e `year`):
```{r}
bd_counties3 = bd_counties2 %>%
    separate(
        col = "var_year",
        into = c("var", "year"), # nomes das colunas separadas
        sep = "_" # caracter que separa as valores na coluna antiga "var_year" 
    )
head(bd_counties3, 10)
```
- Agora, transformaremos a coluna `var` em 2 colunas (`college`, `jobs`), usando a função `pivot_wider()`:
```{r}
bd_counties4 = bd_counties3 %>%
    pivot_wider(
        names_from = "var",
        values_from = "value"
    )
bd_counties4 %>% select(county, year, everything()) %>% head(10)
```
- Observe que, se só houvesse uma variável variante no tempo, não seria necessário usar o `pivot_wider()`, pois haveria 1 linha para cada ano para cada condado.


#### Outro exemplo 2: _long_ $\rightarrow$ _wide_
- Usaremos agora a base de dados `TravelMode` do pacote `AER` que possui 840 observações em que 210 indivíduos escolhem um modo de viagem entre 4 opções: carro, aéreo, trem ou ônibus.
- Note que cada um dos 210 indivíduos aparecem em 4 linhas, em que cada um corresponde a um dos modos de viagem.
- Há variáveis específicas de
    - indivíduo (_individual_, _income_ e _size_) que são repetidas nas 4 linhas em que aparece, e
    - escolha (_choice_, _wait_, _vcost_, _travel_ e _gcost_) que variam de acordo com os modos de viagem.
```{r}
data("TravelMode", package = "AER")
head(TravelMode, 8)
```
- Agora, vamos fazer com que haja apenas uma linha por indivíduo, retirando a coluna _mode_ e gerando diversas colunas para cada possível modo de viagem.
```{r}
TravelMode2 = TravelMode %>% 
    pivot_wider(
        names_from = "mode",
        values_from = c("choice":"gcost") # variáveis específicas do modo
    )
head(TravelMode2)
```
- Note que, para cada modo de viagem, foram criadas 5 colunas, que correspondem às 5 variáveis específicas de escolha. No total, foram retiradas 6 colunas (_mode_ + 5 variáveis específicas de escolha) e foram criadas 20 (4 modos $\times$ 5 variáveis específicas de escolha) colunas.
- Em algumas aplicações econométricas (e.g. logit multinomial) é necessário que haja apenas uma coluna indicando a escolha da opção. Então, criaremos a coluna `choice` indicando qual opção escolheu (_air_, _train_, _bus_ ou _car_) e vamos retirar as 4 colunas que começam com "choice_":
```{r}
TravelMode3 = TravelMode2 %>% 
    mutate(
        choice = case_when(
            choice_air == "yes" ~ "air",
            choice_train == "yes" ~ "train",
            choice_bus == "yes" ~ "bus",
            choice_car == "yes" ~ "car"
        )
    ) %>% select(individual, choice, 
                 starts_with("wait_"), starts_with("vcost_"),
                 starts_with("travel_"), starts_with("gcost_")
                 )

TravelMode3 %>% head(10)
```



## Notações
- Seção 2.1.1 de "Panel Data Econometrics with R" (Croissant \& Millo, 2018)
- A maioria das notações foram adaptadas de acordo com as notas de aula de Econometria I.

Para a observação do indivíduo $i \in \{1, ..., N\}$ no período $t \in \{1, ..., T\}$, podemos escrever o modelo a ser estimado como:
\[ y_{it} = \alpha + x'_{it} \beta + \varepsilon_{it} \]

em que $y_{it}$ é a variável dependente, $x_{it}$ é o vetor de $K$ covariadas, e o erro $\varepsilon_{it}$ pode ser escrito como:
\[ \varepsilon_{it} = u_i + \nu_{it},  \]
sendo $u_i$ o erro individual para o indivíduo $i$ e $\nu_{it}$ é o erro idiossincrático (residual).

Definindo $\gamma \equiv (\alpha, \beta)$ e $Z_{it} \equiv (1, x_{it})$, o modelo pode ser reescrito como
\[ y_{it} = z'_{it} \gamma + \varepsilon_{it}. \]

Para a amostra inteira, supondo que o painel é balanceado, isto é, que possui o mesmo número de observações ($T$) para todos os indivíduos ($i$), temos que
\[ \underbrace{y}_{NT \times 1} = \left( \begin{array}{c}
    y_{11} \\ y_{12} \\ \vdots \\ y_{1T} \\ y_{21} \\ y_{22} \\ \vdots \\ y_{2T} \\ \vdots \\ y_{N1} \\ y_{N2} \\ \vdots \\ y_{NT}
\end{array} \right) \qquad \text{ e } \qquad 
\underbrace{X}_{NT \times K} = \left( \begin{array}{cccc}
    x^1_{11} & x^2_{11} & \cdots & x^K_{11} \\
    x^1_{12} & x^2_{12} & \cdots & x^K_{12} \\
    \vdots & \vdots & \ddots & \vdots \\
    x^1_{1T} & x^2_{1T} & \cdots & x^K_{1T} \\
    x^1_{21} & x^2_{21} & \cdots & x^K_{21} \\
    x^1_{22} & x^2_{22} & \cdots & x^K_{22} \\
    \vdots & \vdots & \ddots & \vdots \\
    x^1_{2T} & x^2_{2T} & \cdots & x^K_{2T} \\
    \vdots & \vdots & \ddots & \vdots \\
    x^1_{N1} & x^2_{N1} & \cdots & x^K_{N1} \\
    x^1_{N2} & x^2_{N2} & \cdots & x^K_{N2} \\
    \vdots & \vdots & \ddots & \vdots \\
    x^1_{NT} & x^2_{NT} & \cdots & x^K_{NT}
\end{array} \right)\]

Denotando $\iota$ um vetor de 1's de tamanho $NT$, temos
\[ y = \alpha \iota + X \beta + \varepsilon \]
ou, usando $Z \equiv (\iota,X)$ e $\gamma \equiv (\alpha, \beta)$,
\[ y = Z \gamma + \varepsilon \]


## Transformações _between_ e _within_
- Seção 2.1.2 de "Panel Data Econometrics with R" (Croissant \& Millo, 2018)

Denote $I_p$ a matriz identidade de dimensão $p$, e $\iota_q$ um vetor de 1's de tamanho $q$. 

A matriz de transformação **inter-indivíduos (_between_)** é denotada por:
\[ B\ =\ I_N \otimes \iota_T (\iota'_T \iota_T)^{-1} \iota'_T \]
Note que a matriz $B$ é equivalente a $N$ nas notas de aula de Econometria I.


Por exemplo, para $N = 2$ e $T = 3$, segue que:
\begin{align*}
    B &= \left( \begin{array}{cc} 1 & 0 \\ 0 & 1 \end{array} \right) \otimes \left[ \left( \begin{array}{c} 1 \\ 1 \\ 1 \end{array} \right) \frac{1}{3} \left( \begin{array}{ccc} 1 & 1 & 1 \end{array} \right) \right] \\
    &= \left( \begin{array}{cc} 1 & 0 \\ 0 & 1 \end{array} \right) \otimes  \left( \begin{array}{ccc} 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & 1/3 \end{array} \right)  \\
    &= \left( \begin{array}{cc} 1 \left( \begin{array}{ccc} 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & 1/3 \end{array} \right) & 0 \left( \begin{array}{ccc} 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & 1/3 \end{array} \right) \\ 0 \left( \begin{array}{ccc} 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & 1/3 \end{array} \right) & 1 \left( \begin{array}{ccc} 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & 1/3 \end{array} \right) \end{array} \right) \\
    &= \left( \begin{array}{rrrrrr} 
        1/3 & 1/3 & 1/3 & 0 & 0 & 0 \\
        1/3 & 1/3 & 1/3 & 0 & 0 & 0 \\
        1/3 & 1/3 & 1/3 & 0 & 0 & 0 \\
        0 & 0 & 0 & 1/3 & 1/3 & 1/3 \\
        0 & 0 & 0 & 1/3 & 1/3 & 1/3 \\
        0 & 0 & 0 & 1/3 & 1/3 & 1/3
    \end{array} \right)_{NT \times NT},
\end{align*}

em que $\otimes$ é o produto de Kronecker. Então, temos
\[ (Bx^k)' = \left( \bar{x}^k_1, \cdots, \bar{x}^k_1, \bar{x}^k_2, \cdots, \bar{x}^k_2, \cdots,\bar{x}^k_N, \cdots, \bar{x}^k_N \right). \]

Para calcular no R, vamos definir:
```{r}
N = 2 # número de indivíduos
T = 3 # números de períodos

iota = matrix(rep(1, T), T, 1) # vetor coluna de 1's de tamanho T
iota

I_N = diag(N) # matriz identidade de tamanho N
I_N
```

Vamos obter $\iota (\iota' \iota)^{-1} \iota'$
```{r}
# Para obter matriz T x T preenchida por 1/T, sendo T = 3, temos que:
t(iota) %*% iota # produto interno de iotas = quantidade T
solve(t(iota) %*% iota) # tomar a inversa = 1/T
iota %*% solve(t(iota) %*% iota) %*% t(iota) # pré e pós-multiplicar por iotas
```

Agora, vamos calcular $B\ =\ I_N \otimes \iota (\iota' \iota)^{-1} \iota'$ usando o operador de produto de Kronecker `%x%`:
```{r}
B = I_N %x% (iota %*% solve(t(iota) %*% iota) %*% t(iota))
round(B, 3)
```

Agora, vamos definir uma matriz de covariadas `X` e pós-multiplicar pela matriz `B`
```{r}
K = 3 # número de covariadas
X = matrix(1:(N*T*K), N*T, K) # matriz covariadas NT x K
Z = cbind(rep(1, N*T), X) # incluindo coluna de 1's
Z

B %*% Z # matriz de médias das covariadas dado indivíduo (NT x K)
```
Note que:

- dada uma variável $k$, temos um único valor (média) dentro de um mesmo indivíduo;
- coluna 1's permaneceu igual após a transformação _between_.


Já a matriz de transformação **intra-indivíduos (_within_)** é dada por:
\[ W\ =\ I_{NT} - B\ =\ I_{NT} - \left[ I_N \otimes \iota (\iota' \iota)^{-1} \iota' \right]. \]

Note que a matriz $W$ é equivalente a $M$ nas notas de aula de Econometria I.

Por exemplo, para $N = 2$ e $T = 3$, segue que:
\begin{align*}
    W &= I_{NT} - B \\
    &= \left( \begin{array}{cccccc} 
        1 & 0 & 0 & 0 & 0 & 0 \\
        0 & 1 & 0 & 0 & 0 & 0 \\
        0 & 0 & 1 & 0 & 0 & 0 \\
        0 & 0 & 0 & 1 & 0 & 0 \\
        0 & 0 & 0 & 0 & 1 & 0 \\
        0 & 0 & 0 & 0 & 0 & 1
    \end{array} \right) - \left( \begin{array}{rrrrrr} 
        1/3 & 1/3 & 1/3 & 0 & 0 & 0 \\
        1/3 & 1/3 & 1/3 & 0 & 0 & 0 \\
        1/3 & 1/3 & 1/3 & 0 & 0 & 0 \\
        0 & 0 & 0 & 1/3 & 1/3 & 1/3 \\
        0 & 0 & 0 & 1/3 & 1/3 & 1/3 \\
        0 & 0 & 0 & 1/3 & 1/3 & 1/3
    \end{array} \right) \\
    &= \left( \begin{array}{rrrrrr} 
         2/6 & -1/3 & -1/3 &    0 &    0 &    0 \\
        -1/3 &  2/6 & -1/3 &    0 &    0 &    0 \\
        -1/3 & -1/3 &  2/6 &    0 &    0 &    0 \\
           0 &    0 &    0 &  2/6 & -1/3 & -1/3 \\
           0 &    0 &    0 & -1/3 &  2/6 & -1/3 \\
           0 &    0 &    0 & -1/3 & -1/3 &  2/6
    \end{array} \right)_{NT \times NT}, 
\end{align*}

```{r}
I_NT = diag(N*T) # matriz identidade com NT elementos na diagonal
W = I_NT - B # matriz de transformação within
W
```
```{r}
round(W %*% Z, 3) # matriz de desvios das médias das covariadas dado indivíduo (NT x K)
```
Observe que:

- dada uma variável $k$, temos os desvios em relação à média de um mesmo indivíduo;
- por isso, a amostra de tamanho $NT$, torna-se uma amostra de $N$ observações distintas;
- coluna 1's virou de 0's após a transformação _within_.
- coluna de 0's, no R, ficou muito próxima de 0 ($1,11 \times 10^{-16}), então foi necessário arredondar.



## Matriz de covariâncias dos erros
- Seção 2.2 de "Panel Data Econometrics with R" (Croissant \& Millo, 2018)

A matriz de covariâncias dos erros depende apenas de dois parâmetros: $\sigma^2_u$ e $\sigma^2_\nu$. Então:

- Variância de um erro: 
\[ E(\varepsilon^2_{it}) = \sigma^2_u + \sigma^2_\nu \]
- Covariância de dois erros de um mesmo indivíduo $i$ em dos períodos  $t \neq s$:
\[ E(\varepsilon_{it} \varepsilon_{is}) = \varepsilon^2_u \]
- Covariância entre dois diferentes indivíduos ($i \neq j$): 
\[ E(\varepsilon_{it} \varepsilon_{jt}) = E(\varepsilon_{it} \varepsilon_{js}) = 0 \]

A matriz de variância dos erros pode ser expressa por:
\[ \Omega = \sigma^2_\nu I_{NT} + \sigma^2_u [I_N \otimes \iota (\iota'\iota)^{-1} \iota'] \]
ou, em termos de $B$ e $W$,
\[ \Omega = \sigma^2_\nu W + T \sigma^2_u B \]


## Estimadores OLS em painel
- Supomos que ambos componentes de erros são não-correlacionados com as covariadas:
\[ E(u|x) = E(\nu|x) = 0 \]
- A variabilidade em um painel tem 2 componentes:
    - a _between_ ou inter-indivíduos, em que a variabilidade das variáveis são mensuradas em médias individuais, como $\bar{z}_i$ ou na forma matricial $BZ$
    - a _within_ ou intra-indivíduos, em que a variabilidade das variáveis são mensuradas em desvios das médias individuais, como $z_{it} - \bar{z}_i$ ou na forma matricial $WZ = Z - BZ$
    - Lembre-se que $z_i \equiv (1, X_i)$ e $Z \equiv (\iota, X)$
- Há três estimadores por OLS que podem ser utilizados:
    1. *Pooled OLS (MQE)*: usando a base de dados bruta (empilhada)
    2. *Estimador Between*: usando as médias individuais
    3. *Estimador Within (Efeitos Fixos)*: usando os desvios das médias individuais


### Pooled OLS (MQE)
- Seção 2.1.1 de "Panel Data Econometrics with R" (Croissant \& Millo, 2018)

O modelo a ser estimado é
\[ y\ =\ Z\gamma + \varepsilon\ =\ \alpha \iota + X \beta + \varepsilon \]

- O estimador $\hat{\gamma} = (\hat{\alpha}, \hat{\beta})$ é dado por
\[ \hat{\gamma}_{OLS} = (Z'Z)^{-1} Z' y \]
- A matriz de covariâncias pode ser obtida usando
\[ V(\hat{\gamma}_{OLS}) = (Z'Z)^{-1} Z' \Omega Z (Z'Z)^{-1} \]



### Estimador _Between_
O modelo a ser estimado é o OLS pré-multiplicado por $B = I_N \otimes \iota (\iota' \iota)^{-1} \iota'$:
\[ By\ =\ BZ\gamma + B\varepsilon\ =\ \alpha \iota + BX \beta + B\varepsilon \]

- O estimador $\hat{\beta}$ é dado por
\[ \hat{\gamma}_{B}\ =\ (Z' B Z )^{-1} Z' B y \]
- A matriz de covariâncias pode ser obtida usando
\begin{align*}
    V(\hat{\gamma}_{B}) &= (Z'BZ)^{-1} Z' B\Omega B Z (Z'BZ)^{-1} \\
    &= \sigma^2_l (Z' B Z)^{-1},
\end{align*}
em que $$\sigma^2_l = \sigma^2_\nu + T \sigma^2_u $$
- O estimador não-viesado de $\sigma^2_l$ é
\[ \hat{\sigma}^2_l = \frac{\hat{\varepsilon}' B \hat{\varepsilon}}{N-K-1} \]
<!-- em que $\hat{q}_B = \ =\ \varepsilon'M_B \varepsilon$. -->
- O estimador _between_ também pode ser estimado por OLS, transformando as variáveis por pré-multiplicação da matriz _between_ ($B$):
\[ \tilde{Z} \equiv BZ \qquad \text{ e } \qquad \tilde{y} = By \] 
tal que 
\[ \hat{\gamma} = ( \tilde{Z}' \tilde{Z} )^{-1} \tilde{Z}' \tilde{y} \]
e assim por diante.
- Note que, a rotina padrão de OLS retorna $\hat{\sigma}^2_l = \frac{\hat{\varepsilon}' B \hat{\varepsilon}}{NT-K-1}$ e, portanto, é necessário fazer ajuste dos graus de liberdade multiplicando a matriz de covariâncias dos erros por $(NT-K-1) / (N-K-1)$.  


### Estimador _Within_ (Efeitos Fixos)
- Não assume que $E(u | X) = 0$
- Estima efeitos individuais para, "limpando" efeito inter-indivíduos nas demais covariadas

O modelo a ser estimado é o OLS pré-multiplicado por $W = I_{NT} - B$:
\[ Wy\ =\ WZ\gamma + W\varepsilon\ =\ WX \beta + W\nu. \]
Note que a transformação within remove vetor de 1's associado ao intercepto, além das covariadas invariantes no tempo e o termo de erro individual $u$ (sobrando apenas $\varepsilon = \nu$).

- O estimador $\hat{\beta}$ é dado por
\[ \hat{\beta}_{W}\ =\ (X' W X )^{-1} X' W y \]
- A matriz de covariâncias pode ser obtida usando
\begin{align*}
    V(\hat{\beta}_{W}) &= (X'WX)^{-1} X' W\Omega W X (X'WX)^{-1} \\
    &= \sigma^2_\nu (X' W X)^{-1}.
\end{align*}
- O estimador não-viesado de $\sigma^2_\nu$ é
\[ \hat{\sigma}^2_\nu = \frac{\hat{\varepsilon}' W \hat{\varepsilon}}{NT-K-N} \]
<!-- em que $\hat{q}_W = \ =\ \varepsilon'M_W \varepsilon$. -->
- O estimador _within_ também pode ser estimado por OLS, transformando as variáveis por pré-multiplicação da matriz _within_ ($W$):
\[ \tilde{Z} \equiv WZ \qquad \text{ e } \qquad \tilde{y} = Wy \] 
tal que 
\[ \hat{\gamma} = ( \tilde{Z}' \tilde{Z} )^{-1} \tilde{Z}' \tilde{y} \]
e assim por diante.
- Note que, a rotina padrão de OLS retorna $\hat{\sigma}^2_\nu = \frac{\hat{\varepsilon}' W \hat{\varepsilon}}{NT-K-1}$ e, portanto, é necessário fazer ajuste dos graus de liberdade multiplicando a matriz de covariâncias dos erros por $(NT-K-1) / (NT-K-N)$.


### Estimação OLS em painel
#### Estimação via `plm()`
Para ilustrar as estimações OLS dos estimadores vistos anteriormente, usaremos a base de dados `TobinQ` do pacote `pder`, que conta com dados de 188 firmas americanos por 35 anos (6580 observações).
```{r}
library(dplyr)
data("TobinQ", package = "pder")
summary(TobinQ %>% select(cusip, year, ikn, qn))
```
- `cusip`: Identificador da empresa
- `year`: Ano
- `ikn`: Investimento dividido pelo capital
- `qn`: Q de Tobin (razão entre valor da firma e o custo de reposição de seu capital físico). Se $Q > 1$, então o lucro investimento é maior do que seu custo.

Queremos estimar o seguinte modelo:
\[ \text{ikn} = \alpha + \text{qn} \beta + \varepsilon \]


Usaremos a função `plm()` (do pacote de mesmo nome) para estimar modelos lineares em dados em painel. Seus principais argumentos são:

- `formula`: equação do modelo
- `data`: base de dados em `data.frame` (precisa preencher `index`) ou `pdata.frame` (formato próprio do pacote que já indexa as colunas de indivíduos e de tempo)
- `model`: estimador a ser computado 'pooling' (MQE), 'between', 'within' (Efeitos Fixos) e 'random' (Efeitos Aleatórios/GLS)
- `index`: vetor de nomes dos identificadores de indivíduo e de tempo

```{r warning=FALSE}
library(plm)

# Transformando no formato pdata frame, com indentificador de indivíduo e de tempo
pTobinQ = pdata.frame(TobinQ, index=c("cusip", "year"))

# Estimações
Q.pooling = plm(ikn ~ qn, pTobinQ, model = "pooling")
Q.between = plm(ikn ~ qn, pTobinQ, model = "between")
Q.within = plm(ikn ~ qn, pTobinQ, model = "within")

summary(Q.within) # output da estimação within

# Resumindo 3 estimações em única tabela
stargazer::stargazer(Q.pooling, Q.between, Q.within, type="text")
```
- Observe que:
    - as variáveis entram ns estimação sem nenhuma transformação as diferentes quantidades de observações e
    - cada método possui diferentes graus de liberdade


##### Estimando _Within_ e _Between_ via Pooled OLS
- Nós podemos construir as variáveis de média e de desvios de média diretamente no data frame e estimar o _between_ e _within_ via (pooled) OLS
```{r}
TobinQ = TobinQ %>% group_by(cusip) %>% # agrupando por cusip
    mutate(
        ikn_bar = mean(ikn), # "transformação" between de ikn
        qn_bar = mean(qn), # "transformação" between de qn
        ikn_desv = ikn - ikn_bar, # "transformação" within de ikn
        qn_desv = qn - qn_bar # "transformação" within de qn
    ) %>% ungroup()

pTobinQ = pdata.frame(TobinQ, index=c("cusip", "year"))

Q.pooling_between = plm(ikn_bar ~ qn_bar, pTobinQ, model = "pooling")

summary(Q.pooling_between)$coef # between via pooled OLS
summary(Q.between)$coef # between
```
- Note que, embora as estimativas sejam as mesmas, acabamos subestimando os erros padrão e, portanto, superestimando os valores t.
- Ao estimar o _between_ via pooled OLS, ele não faz os ajustes dos graus de liberdade nas variâncias das estimativas
- Logo, vamos ajustar os graus de liberdade multiplicando a variância das estimativas por $(NT - K - 1)$ e dividindo por $(N - K - 1)$
```{r}
std_error = summary(Q.pooling_between)$coef[, "Std. Error"]
variance = std_error^2
adj_variance = variance * (188*35 - 1 - 1) / (188 - 1 - 1)
adj_std_error = sqrt(adj_variance)
adj_std_error
```


##### Efeitos Fixos da Estimação _Within_
- Para o estimador _within_, podemos usar a função `fixef()` para computar os efeitos individuais. É possível calcular 3 tipos por meio do argumento `type`:
    - `level`: valor padrão que retorna os interceptos individuais ($\hat{\alpha} + \hat{u}_{i}$)
    - `dfirst`: em desvios do 1º indivíduo ($\hat{\alpha}$ é o intercepto do 1º indivíduo)
    - `dmean`: em desvios da média de efeitos individuais ($\hat{\alpha}$ é a média)

```{r}
# 6 primeiros efeitos individuais de cada tipo
for (t in c("level", "dfirst", "dmean")) {
    print( head( fixef(Q.within, type=t) ) )
}
```
- Note que, como o `dfirst` retorna valores em relação ao 1º indivíduo, este não aparece no output do `fixef()`, diferente dos demais.
- No caso linear, o estimador _within_ é equivalente à estimação por OLS com inclusão de dummies para cada indivíduo:
```{r}
# Estimando OLS com dummies individuais - factor() tranforma cusip em var. categ.
Q.dummies = lm(ikn ~ qn + factor(cusip), pTobinQ)

# Comparando as estimativas de qn
cbind(Q.within$coef, Q.dummies$coef["qn"])

# Comparando efeitos fixos (dfirst) e dummies
cbind(head(fixef(Q.within, type="dfirst")),
      Q.dummies$coef[3:8])
```


#### Estimação Analítica Pooled OLS (MQE)
Igual a estimação analítica de MQO vista anteriormente.


#### Estimação Analítica _Between_
```{r}
data("TobinQ", package="pder")
TobinQ = TobinQ %>% mutate(constant = 1) # criando vetor de 1's

y = TobinQ %>% select(ikn) %>% as.matrix() # vetor y
X = TobinQ %>% select(qn) %>% as.matrix() # vetor X
Z = cbind(TobinQ$constant, X) # vetor Z = (iota, X)

N = TobinQ %>% select(cusip) %>% unique() %>% nrow()
T = TobinQ %>% select(year) %>% unique() %>% nrow()
iota_T = rep(1, T)

# Calculando matrizes de tranformação B e W
B = diag(N) %x% (iota_T %*% solve(t(iota_T) %*% iota_T) %*% t(iota_T))
W = diag(N*T) - B
```


\[ \hat{\gamma} = (\hat{\alpha}, \hat{\beta}) = (Z' B Z)^{-1} Z' By  \]

```{r}
# vetor de estimativas gamma_hat = (alpha, beta)
gamma_hat = solve(t(Z) %*% B %*% Z) %*% t(Z) %*% B %*% y
gamma_hat
```


\[ \hat{y} = Z \hat{\gamma} \qquad \text{ e } \qquad  \hat{\varepsilon} = y - \hat{y} \]
```{r}
# valores ajustados e erros
y_hat = Z %*% gamma_hat
e_hat = y - y_hat
```


\[ \hat{\sigma}^2 = \frac{\hat{\varepsilon}' B \hat{\varepsilon}}{N-K-1} \]
```{r}
## Estimando variancia do termo de erro
sigma2_l = t(e_hat) %*% B %*% e_hat / (N - ncol(Z)) # N - K - 1 graus de liberdade!
sigma2_l
```
**IMPORTANTE**: Ajustar os graus de liberdade do estimador _between_ para $N - K - 1$ (ao invés de $NT - K - 1$)

\[ \widehat{V}(\hat{\gamma}) = \hat{\sigma}^2_l (Z'BZ)^{-1} \]
```{r}
## Estimando a matriz de variancia/covariancia das estimativas gamma
vcov_hat = c(sigma2_l) * solve(t(Z) %*% B %*% Z)
vcov_hat

## Calculando erros padrao das estimativas gamma
std_error = sqrt(diag(vcov_hat)) # Raiz da diagonal da matriz de covariâncias

## Calculando estatisticas t das estimativas gamma
t_stat = gamma_hat / std_error

## Calculando p-valores das estimativas gamma
p_value = 2 * pt(q = -abs(t_stat), df = N - ncol(Z))  # N - K - 1 graus de liberdade!

## Organizando os resultados da regressao em uma matriz
results = cbind(gamma_hat, std_error, t_stat, p_value)

## Nomeando as colunas da matriz de resultados
colnames(results) = c('Estimate', 'Std. Error', 't stat', 'Pr(>|t|)')
results

summary(Q.between)$coef # comparando com estimado via plm()
```


#### Estimação Analítica _Within_
(Exercício)


## Estimadores GLS em painel
- Seção 2.3 de "Panel Data Econometrics with R" (Croissant \& Millo, 2018)
- Ao contrário do estimador _within_ que retira os efeitos individuais, o estimador de **GLS** considera que os efeitos individuais como aleatórios a partir de uma distribuição específica.
- Erros não são correlacionados com as covariadas, e são caracterizados por uma matriz de covariâncias $\Omega$.
- O estimador de GLS é dado por
\[ \hat{\gamma}_{GLS} = (Z' \Omega^{-1} Z)^{-1} (Z' \Omega^{-1} y) \tag{2.27} \]
- A variância do estimador é dada por
\[ V(\hat{\gamma}_{GLS}) = (Z' \Omega^{-1} Z)^{-1} \tag{2.28} \]
- A matriz $\Omega$ depende apenas de dois parâmetros: $\sigma^2_u$ e $\sigma^2_\nu$, temos:
\[ \Omega^p = (\sigma^2_l)^p B + (\sigma^2_\nu)^p W \tag{2.29} \]
<!-- - Usando $p=-1$ e substituindo $\Omega^{-1}$ em (2.27) e (2.28), temos: -->
<!-- \begin{align*} -->
<!--     \hat{\gamma}_{GLS} &= \left(\frac{1}{\sigma^2_\nu} Z' W Z + \frac{1}{\sigma^2_l} Z' B Z \right)^{-1} \left(\frac{1}{\sigma^2_\nu} Z' W y + \frac{1}{\sigma^2_l} Z' B y \right) \tag{2.30} \\ -->
<!--     V(\hat{\gamma}_{GLS}) &= \left(\frac{1}{\sigma^2_\nu} Z' W Z + \frac{1}{\sigma^2_l} Z' B Z  \right)^{-1} \tag{2.31} -->
<!-- \end{align*} -->
- Lembre-se que $$\sigma^2_l = \sigma^2_\nu + T \sigma^2_u $$

### GLS: combinação de Pooled OLS (Ef. Aleatórios) e de _Within_ (Ef. Fixos)
- Pode-se computar mais eficientemente por OLS, que necessita transformação das variáveis usando a matriz $\Omega^{-0.5}$, tal que $\Omega^{-0.5\prime}\Omega^{-0.5} = \Omega^{-1}$.
- Denotando $\tilde{y} \equiv \Omega^{-0.5}y$ e $\tilde{Z} \equiv \Omega^{-0.5}Z$, o modelo com variáveis transformadas é dado por
\begin{align*}
    \hat{\gamma} &= (Z' \Omega^{-1} Z)^{-1} (Z' \Omega^{-1} y) \tag{2.27} \\
    &= (Z' \Omega^{-0.5\prime} \Omega^{-0.5} Z)^{-1} (Z' \Omega^{-0.5}\Omega^{-0.5\prime} y) \\
    &= (\tilde{Z}'\tilde{Z})^{-1} \tilde{Z} \tilde{y}
\end{align*}

Usando (2.29), $p=-0.5$ em (2.29), tem-se
\[ \Omega^{-0.5} = \frac{1}{\sigma_l} B + \frac{1}{\sigma_\nu} W \]

Essa transformação evidencia uma combinação linear entre as matrizes de transformação _between_ e _within_ ponderadas pelo inverso dos desvios padrão dos 2 componentes de erro ($\sigma^2_\nu$ e $\sigma^2_u = (\sigma^2_\nu + \sigma^2_l)/T$)

Pré-multiplicando as variáveis por $\sigma_\nu \Omega^{-0.5}$ (ao invés de $\Omega^{-0.5}$ para simplificação e sem perda de generalidade), as covariadas transformadas para o indivíduo $i$ no tempo $t$ são dadas por:
\[ \tilde{z}_{it}\ =\ \frac{\sigma_\nu}{\sigma_l} \bar{z}_{i\cdot} + (z_{it} - \bar{z}_{i\cdot})\ =\ z_{it} + \left(1 - \frac{\sigma_\nu}{\sigma_l} \right) \bar{z}_{i\cdot}\ \equiv\ z_{it} - \theta \bar{z}_{i\cdot} \]
em que
\[ \theta\ \equiv\ 1 - \frac{\sigma_\nu}{\sigma_l}\ \equiv\ 1 - \phi \]
    <!-- &= 1 - \frac{(\sqrt{\sigma_\nu})^2}{\sqrt{\sigma^2_\nu + T \sigma^2_u}}  \tag{$\sigma^2_l = \sigma^2_\nu + T \sigma^2_u$} \\ -->
    <!-- &= 1 - \frac{1}{\sqrt{1 + T \frac{\sigma^2_u}{\sigma^2_\nu}}} -->

    
Note que, quando:

- $\theta \rightarrow 1$, os efeitos individuais $\sigma_u$ dominam $\implies$ GLS se aproxima do estimador _within_
- $\theta \rightarrow 0$, os efeitos individuais $\sigma_u$ somem $\implies$ GLS se aproxima do pooled OLS


### Estimação dos Componentes de Erro $\sigma_u$ e $\sigma_\nu$
- Note que não temos $\sigma^2_\nu$ e $\sigma^2_u = (\sigma^2_\nu + \sigma^2_l)/T$ e, logo, $\Omega$ é desconhecido.
- Se $\varepsilon$ fosse conhecido, então os estimadores para as duas variâncias seriam:
\begin{align*}
    \hat{\sigma}^2_l &= \frac{\varepsilon' B \varepsilon}{N} \tag{2.34}\\
    \hat{\sigma}^2_\nu &= \frac{\varepsilon' W \varepsilon}{N(T-1)} \tag{2.35}
\end{align*}
- Como $\varepsilon$ é desconhecido, então usam-se resíduos de estimadores consistentes em seu lugar.
- O estimador obtido por esse processo é chamado de FGLS (ou GLS Factível)
- **Wallace e Hussain (1969)**: usam resíduos OLS
\begin{align*}
    \hat{\sigma}^2_l &= \frac{\hat{\varepsilon}'_{OLS} B \hat{\varepsilon}_{OLS}}{N} \\
    \hat{\sigma}^2_\nu &= \frac{\hat{\varepsilon}'_{OLS} W \hat{\varepsilon}_{OLS}}{N(T-1)}
\end{align*}
- **Amemiya (1971)**: usam resíduos _within_
\begin{align*}
    \hat{\sigma}^2_l &= \frac{\hat{\varepsilon}'_{W} B \hat{\varepsilon}_{W}}{N}\\
    \hat{\sigma}^2_\nu &= \frac{\hat{\varepsilon}'_{W} W \hat{\varepsilon}_{W}}{N(T-1)}
\end{align*}
Note que, a variância do efeito individual é sobre-estimado quando o modelo contém variáveis invariantes no tempo (somem com a transformação _within_)
- **Hausman e Taylor (1981)**: propuseram ajuste ao método de Amemiya (1971), em que $\hat{\varepsilon}_W$ são regredidos em todas variáveis invariantes no tempo no modelo e são utilizados os resíduos dessa regressão, $\hat{\varepsilon}_{HT}$.
- **Swamy e Arora (1972)**: usam resíduos _between_ e _within_ para calcular, respectivamente, $\hat{\sigma}^2_l$ e $\hat{\sigma}^2_\nu$
\begin{align*}
    \hat{\sigma}^2_l &= \frac{\hat{\varepsilon}'_{B} B \hat{\varepsilon}_{B}}{N - K - 1}\\
    \hat{\sigma}^2_\nu &= \frac{\hat{\varepsilon}'_{W} W \hat{\varepsilon}_{W}}{N(T-1) - K}
\end{align*}
- **Nerlove (1971)**: computam $\sigma^2_u$ empírica dos efeitos fixos do modelo _within_
\begin{align*}
    \hat{u}_i &= \bar{y}_{i\cdot} - \hat{\beta}_W \bar{x}_{i\cdot} \\
    \hat{\sigma}^2_u &= \sum^N_{i=1}{(\hat{u}_i - \bar{\hat{u}}) / (N-1)} \\
    \hat{\sigma}^2_\nu &= \frac{\hat{\varepsilon}'_W W \hat{\varepsilon}_W}{NT}
\end{align*}


### Estimação GLS via `plm()`
- Usaremos novamente a função `plm()`, mas definiremos `model = random` para que seja estimado via GLS
- em `random.method` podemos escolher o método de cálculo dos parâmetros de erro:
    1. `"walhus"` para Wallace e Hussain (1969)
    2. `"amemiya"` para Amemiya (1971)
    3. `"ht"` para Hausman e Taylor (1981)
    4. `"swar"` para Swamy e Arora (1972) [padrão]
    5. `"nerlove"` para Nerlove (1971)

```{r}
library(plm)
data("TobinQ", package = "pder")
pTobinQ = pdata.frame(TobinQ, index=c("cusip", "year"))

# Estimações GLS
Q.walhus = plm(ikn ~ qn, pTobinQ, model = "random", random.method = "walhus")
Q.amemiya = plm(ikn ~ qn, pTobinQ, model = "random", random.method = "amemiya")
Q.ht = plm(ikn ~ qn, pTobinQ, model = "random", random.method = "ht")
Q.swar = plm(ikn ~ qn, pTobinQ, model = "random", random.method = "swar")
Q.nerlove = plm(ikn ~ qn, pTobinQ, model = "random", random.method = "nerlove")

summary(Q.walhus) # output da estimação GLS por Wallace e Hussain (1969)
```
Note que $\theta = 73\%$, o que indica que, neste caso, o estimativa GLS é mais próxima de _within_ ($\theta=1$) do que de _between_ ($\theta=0$). A grande quantidade de períodos ($T = 35$) provavelmente influencia este alto valor.


```{r}
# Resumindo 5 estimações em única tabela
stargazer::stargazer(Q.walhus, Q.amemiya, Q.ht, Q.swar, Q.nerlove, type="text")
```
Os resultados são praticamente idênticos, assim como seus $\theta$'s:

```{r}
# Podemos visualizar o theta usando ercomp()$theta
ercomp(Q.walhus)$theta

# Criaremos uma lista com todos objetos de estimação GLS
Q.models = list(walhus = Q.walhus, amemiya = Q.amemiya, ht = Q.ht,
                swar = Q.swar, nerlove = Q.nerlove)

# Aplicaremos sapply() com a lista criada e a função ercomp()
sapply(Q.models, function(model) ercomp(model)$theta)
```


Observe que poderíamos ter obtido informações sobre as variâncias de covariadas por meio de `summary()` sobre uma variável no formato `pdata.frame`:
```{r}
pTobinQ = pdata.frame(TobinQ, index=c("cusip", "year")) # transf. em pdata.frame
summary(pTobinQ$qn) # resumo sobre variável qn
```

Também é possível verificar o mesmo para a variância do erro na estimação em GLS:
```{r}
ercomp(ikn ~ qn, TobinQ) # padrão method = "swar"
```



#### Estimação Analítica GLS
- Aqui, faremos a estimação analítica do GLS usando o método de Wallace e Hussain (1969).
- Consiste no uso dos desvios estimados por pooled OLS para calcular $\hat{\sigma}^2_l$, $\hat{\sigma}^2_\nu$ (e consequentemente $\hat{\sigma}^2_u$), possibilitando encontrar $\hat{\Omega}^{-1}$ para estimar por FGLS.
- Para agilizar a estimação, vamos estimar o pooled OLS por `plm()`:
```{r}
library(plm)
data("TobinQ", package = "pder")

# Transformando no formato pdata frame, com indentificador de indivíduo e de tempo
pTobinQ = pdata.frame(TobinQ, index=c("cusip", "year"))
```

- Obtendo $\hat{\varepsilon}_{OLS}$

```{r}
# Estimação pooled OLS
Q.pooling = plm(ikn ~ qn, pTobinQ, model = "pooling")

# obtendo os resíduos OLS
e_OLS = Q.pooling$residuals %>% as.vector()
head(e_OLS)
```

- Precisamos calcular $\hat{\sigma}^2_l$, $\hat{\sigma}^2_\nu$ e $\hat{\sigma}^2_u$

```{r}
TobinQ = TobinQ %>% mutate(constant = 1) # criando vetor de 1's

y = TobinQ %>% select(ikn) %>% as.matrix() # vetor y
X = TobinQ %>% select(qn) %>% as.matrix() # vetor X
Z = cbind(TobinQ$constant, X) # vetor Z = (iota, X)

N = TobinQ %>% select(cusip) %>% unique() %>% nrow() # núm. indivíduos
T = TobinQ %>% select(year) %>% unique() %>% nrow() # núm. períodos
iota_T = rep(1, T)

# Calculando matrizes de tranformação B e W
B = diag(N) %x% (iota_T %*% solve(t(iota_T) %*% iota_T) %*% t(iota_T))
W = diag(N*T) - B

# Calculando os termos de erro
sigma2_l = (t(e_OLS) %*% B %*% e_OLS) / N
sigma2_nu = (t(e_OLS) %*% W %*% e_OLS) / (N * (T-1))
sigma2_u = (sigma2_l + sigma2_nu) / T

sigmas2 = cbind(sigma2_l, sigma2_nu, sigma2_u)
colnames(sigmas2) = c("sigma2_l", "sigma2_nu", "sigma2_u")
sigmas2
```

- Agora, conseguimos calcular:
\[ \Omega^{-1} = \frac{1}{\sigma^2_l}B + \frac{1}{\sigma^2_\nu}W \]
```{r}
Omega_1 = c(sigma2_l^(-1)) * B + c(sigma2_nu^(-1)) * W
dim(Omega_1) # NT x NT
```

\[ \hat{V}_{GLS} = (Z' \Omega^{-1} Z)^{-1} \]
<!-- \[ \hat{y} = Z\hat{\gamma} \qquad \text{e} \qquad \varepsilon = y - \hat{y} \] -->

```{r}
## Estimando a matriz de variancia/covariancia das estimativas gamma
vcov_hat = solve(t(Z) %*% Omega_1 %*% Z)

# vetor de estimativas gamma_hat = (alpha, beta)
gamma_hat = solve(t(Z) %*% Omega_1 %*% Z) %*% (t(Z) %*% Omega_1 %*% y)
gamma_hat

## Calculando erros padrao das estimativas gamma
std_err = sqrt(diag(vcov_hat)) # Raiz da diagonal da matriz de covariâncias

## Calculando estatisticas t das estimativas gamma
t_stat = gamma_hat / std_err

## Calculando p-valores das estimativas gamma
p_value = 2 * pt(q = -abs(t_stat), df = nrow(Z) - ncol(Z))  # NT - K - 1 graus de liberdade

## Organizando os resultados da regressao em uma matriz
results_walhus = cbind(gamma_hat, std_err, t_stat, p_value)

## Nomeando as colunas da matriz de resultados
colnames(results_walhus) = c('Estimate', 'Std. Error', 't stat', 'Pr(>|t|)')
rownames(results_walhus) = c("(Intercept)", "qn")
results_walhus

summary(Q.walhus)$coef # comparando com estimado via plm()
```

## Comparativo dos Estimadores OLS e GLS - Exemplo
- Seção 2.4.4 de "Panel Data Econometrics with R" (Croissant \& Millo, 2018)
- Usado por Kinal e Lahiri (1993) 
- Queremos estabelecer relação entre importações (_imports_) e produto nacional (_gnp_)
```{r}
data("ForeignTrade", package = "pder")
FT = pdata.frame(ForeignTrade, index=c("country", "year"))

# Variâncias 
ercomp(imports ~ gnp, FT) # variância do erro na estimação GLS
```
- Variância do erro da estimação GLS é dada por 93\% de variação inter-indivíduos
- O estimador GLS remove grande parte da variação inter-indivíduos, pois subtrai, da covariada, 94\% da média individual:
\[ \tilde{z}_{it}\ =\ z_{it} + \left(1 - \frac{\sigma_\nu}{\sigma_l} \right) \bar{z}_{i\cdot}\ \equiv\ z_{it} - \theta \bar{z}_{i\cdot}\ =\ z_{it} - 0,94 \bar{z}_{i\cdot} \]

```{r}
# Estimações
models = c("within", "random", "pooling", "between")
sapply(models, function(x) round(
    coef(summary(plm(imports ~ gnp, FT, model=x)))["gnp",], 4))
```

```{r, echo=FALSE}
# Define variable containing url
url = "https://fhnishida.github.io/fearp/eco1/example_panel-1.png"
```
<center><img src="`r url`"></center>

- GLS e _within_ são bastante parecidos
- OLS, que considera variação inter-indivíduos, é parecido com _between_

## Estimador ML em painel
- Seção 3.3 de "Panel Data Econometrics with R" (Croissant \& Millo, 2018)
- Uma alternativa aos estimadores de GLS é o de máxima verossimilhança (ML).
- Assume-se que a distribuição dos dois componentes de erro são normais:
\[ u | X \sim N(0, \sigma^2_u) \quad \text{ e } \quad v | u, X \sim N(0, \sigma^2_\nu) \]
- O modelo a ser estimado é o
\[ y_{it} = \alpha \iota + \beta' x_i + u_i + \nu_{it} = \gamma' z_i + u_i + \nu_{it} \]
- Ao invés de estimar $\sigma^2_u$ e $\sigma^2_\nu$ para depois calcular $\gamma$, ambos são estimados simultaneamente.
- Denotando $\phi = \frac{\sigma_\nu}{\sigma_{l}}$, a função de log-verossimilhança para um painel balanceado é:
\[ \ln{L} = -\frac{NT}{2} \ln{2\pi} - \frac{NT}{2}\ln{\sigma^2_\nu} + \frac{N}{2} \ln{\phi^2} - \frac{1}{2\sigma^2_\nu} \left( \varepsilon' W \varepsilon + \phi^2 \varepsilon' B \varepsilon \right) \]
- Denotando $$\tilde{Z}\ \equiv\ (I - \phi B) Z\ =\ Z - \phi B Z$$ e resolvendo as CPO's da log-verossimilhança, segue que:
\begin{align*}
    \hat{\gamma} &= (\tilde{Z}'\tilde{Z})^{-1} \tilde{Z}'\tilde{y} \tag{3.12} \\
    \hat{\sigma}^2_\nu &= \frac{\hat{\varepsilon}' W \hat{\varepsilon} + \hat{\phi}^2 \hat{\varepsilon}' B \hat{\varepsilon}}{NT} \tag{3.13} \\
    \hat{\phi}^2 &=\frac{\hat{\varepsilon}' W \hat{\varepsilon}}{(T-1) \hat{\varepsilon}'B\hat{\varepsilon}} \tag{3.14}
\end{align*}
- A estimação pode ser feita iterativamente por FIML (Full Information Maximum Likelihood):
    1. Chute inicial de $\hat{\gamma}$ (por exemplo, estimativa _within_)
    2. Calcular $\hat{\phi}^2$ usando (3.14)
    3. Calcular $\hat{\gamma}$ usando (3.12) 
    4. Verificar convergência: se não convergiu, volta para o passo 2, usando o $\hat{\gamma}$ calculado no passo 3.
    5. Calcular $\sigma^2_\nu$ usando (3.13)

### Estimação ML via `pglm()`
```{r warning=FALSE}
library(pglm)
library(dplyr)
data("TobinQ", package = "pder")

# Transformando no formato pdata frame, com indentificador de indivíduo e de tempo
pTobinQ = pdata.frame(TobinQ, index=c("cusip", "year"))

# Estimação pooled OLS
Q.ml = pglm(ikn ~ qn, pTobinQ, family = "gaussian")
summary(Q.ml)

summary(Q.swar)$coef # Comparando com estimação GLS-swar
```
- Note que o resultado por ML foi bem próximo ao do obtido por GLS


<!-- ### Estimação Numérica ML -->
<!-- ```{r} -->
<!-- TobinQ = TobinQ %>% mutate(constant = 1) # criando vetor de 1's -->

<!-- y = TobinQ %>% select(ikn) %>% as.matrix() # vetor y -->
<!-- X = TobinQ %>% select(qn) %>% as.matrix() # vetor X -->
<!-- Z = cbind(TobinQ$constant, X) # vetor Z = (iota, X) -->

<!-- N = TobinQ %>% select(cusip) %>% unique() %>% nrow() # núm. indivíduos -->
<!-- T = TobinQ %>% select(year) %>% unique() %>% nrow() # núm. períodos -->
<!-- iota_T = rep(1, T) -->

<!-- # Calculando matrizes de tranformação B e W -->
<!-- B = diag(N) %x% (iota_T %*% solve(t(iota_T) %*% iota_T) %*% t(iota_T)) -->
<!-- W = diag(N*T) - B -->
<!-- ``` -->

<!-- Agora, iremos realizar iterações até a convergência de $\hat{\gamma}$ -->
<!-- ```{r} -->
<!-- gamma_ini = c(0, 0) # chute inicial -->
<!-- tol = 1e-10 # tolerância para convergência -->
<!-- dist = 1 # distância inicial - apenas para entrar no loop/while -->
<!-- it = 0 # número de iterações -->

<!-- while (dist > tol) { -->
<!--     print(paste0("iteração ", it, -->
<!--                  ": alpha = ", round(gamma_ini[1], 7), -->
<!--                  " | beta = ", round(gamma_ini[2], 7))) -->

<!--     # Encontrando erros e transformando em vetor -->
<!--     y_hat = Z %*% gamma_ini -->
<!--     e = as.vector(y - y_hat) -->

<!--     # Calculando \phi^2 -->
<!--     phi2_hat = (t(e) %*% W %*% e) / ((T-1) * (t(e) %*% B %*% e)) -->
<!--     phi_hat = as.vector(sqrt(phi2_hat)) -->

<!--     # Transformando variáveis -->
<!--     Z_til = Z - (phi_hat * B) %*% Z -->
<!--     y_til = y - (phi_hat * B) %*% y -->

<!--     # Calculando estimativas dos parâmetros -->
<!--     gamma_fim = solve(t(Z_til) %*% Z_til) %*% t(Z_til) %*% y_til -->

<!--     # Calculando distância entre as estimativas -->
<!--     dist = max(abs(gamma_fim - gamma_ini)) -->
<!--     gamma_ini = gamma_fim -->
<!--     it = it + 1 -->
<!-- } -->
<!-- ``` -->
<!-- ```{r} -->
<!-- y_hat = Z %*% gamma_fim -->
<!-- e = as.vector(y - y_hat) -->
<!-- phi2_hat = (t(e) %*% W %*% e) / ((T-1) * (t(e) %*% B %*% e)) -->
<!-- phi_hat = sqrt(phi2_hat) -->

<!-- # Calculando desvio padrão idiossincrático \nu -->
<!-- sigma2_nu = (t(e) %*% W %*% e  +  phi2_hat * t(e) %*% B %*% e) / (N*T) -->
<!-- print(paste0("sd_idios = ", round(sqrt(sigma2_nu), 6))) -->
<!-- ``` -->
<!-- Como $\phi = \sigma_\nu / \sigma_l$ e $\sigma^2_\nu = (\sigma^2_\nu + \sigma^2_l) / T$, então: -->
<!-- ```{r} -->
<!-- sigma_l = sqrt(sigma2_nu) / phi_hat -->
<!-- sigma2_u = (sigma_l^2 + sigma2_nu) / T -->
<!-- print(paste0("sd_id = ", round(sqrt(sigma2_u), 6))) -->
<!-- ``` -->
<!-- Observe que obtivemos resultados próximos do obtido via `pglm()`. -->


## Testes de Presença de Efeitos Individuais

### Testes Breusch-Pagan
- Seção 4.1 de "Panel Data Econometrics with R" (Croissant \& Millo, 2018)
- É um teste baseado em multiplicadores de Lagrange (LM) nos resíduos de OLS, em que $H_0: \sigma^2_u = 0$ (ausência de efeitos individuais)
- A estatística teste é dada por 
\[ LM_u = \frac{NT}{2(T-1)} \left( T \frac{\hat{\varepsilon}' B_u \hat{\varepsilon}}{\hat{\varepsilon}' \hat{\varepsilon}} - 1 \right)^2  \]
que é assintoticamente distribuída como ua $\chi^2$ com 1 grau de liberdade.
- Há algumas variações deste teste:
    - Breusch and Pagan (1980),
    - Gourieroux et al. (1982),
    - Honda (1985), e
    - King and Wu (1997).



### Testes F
- Seção 4.1 de "Panel Data Econometrics with R" (Croissant \& Millo, 2018)
- Sejam a soma dos resíduos ao quadrado e os graus de liberdade do modelo _within_ $\hat{\varepsilon}'_W\hat{\varepsilon}_W$ e $N(T-1) - K$, respectivamente.
- Sejam a soma dos resíduos ao quadrado e os graus de liberdade do modelo pooled OLS $\hat{\varepsilon}'_{OLS}\hat{\varepsilon}_{OLS}$ e $NT - K - 1$, respectivamente.
- Sob hipótese nula de que não há efeitos individuais, a estatística teste é dada por
\[ \frac{\hat{\varepsilon}'_{OLS} W \hat{\varepsilon}_{OLS} - \hat{\varepsilon}'_W\hat{\varepsilon}_W}{\hat{\varepsilon}'_W W \hat{\varepsilon}_W} \frac{NT - K - N + 1}{N-1} \]
que segue uma distribuição F de Fisher-Snedecor com $N-1$ e $NT - K - N + 1$ graus de liberdade.


### Implementando os Testes no R
```{r}
data("TobinQ", package = "pder")
pTobinQ = pdata.frame(TobinQ, index=c("cusip", "year"))

Q.within = plm(ikn ~ qn, pTobinQ, model = "within")
Q.gls = plm(ikn ~ qn, pTobinQ, model = "random")
Q.pooling = plm(ikn ~ qn, pTobinQ, model = "pooling")

# Teste de Breusch-Pagan/LM
plmtest(Q.pooling, effect="individual") # Honda (1985)
```
O teste LM (Breusch-Pagan) acusou efeitos individuais significativos.

```{r}
# Teste F
pFtest(Q.within, Q.pooling)
```
Assim como o teste LM, Pelo teste F, observam-se efeitos individuais significativos.


## Testes de Efeitos Correlacionados
- Seção 4.2 de "Panel Data Econometrics with R" (Croissant \& Millo, 2018)
- Continuamos assumindo que $E(\nu|X) = 0$, em que $\nu$ é o termo de erro idiossincrático.
- Nestes testes, verificamos se $E(u|X) = 0$, ou seja, se os efeitos individuais são ou não são correlacionados com as covariadas.

### Teste de Hausman
- O princípio geral do teste de Hausman consiste em comparar dois modelos $A$ e $B$ tal que
    - sob $H_0$: $A$ e $B$ são ambos consistentes, mas $B$ é mais eficiente que $A$
    - sob $H_1$: apenas $A$ é consistente
- Se $H_0$ é verdadeiro, então os coeficientes dos dois modelos não devem divergir.
- O teste é baseado em $\hat{\beta}_A - \hat{\beta}_B$ e Hausman mostrou que, sob $H_0$, temos $cov(\hat{\beta}_A, \hat{\beta}_B) = 0$ e, logo, a variância dessa diferença é simplesmente $V(\hat{\beta}_A - \hat{\beta}_B) = V(\hat{\beta}_A) - V(\hat{\beta}_B)$

- No contexto de dados em painéis, compara-se o estimador _within_ (efeitos fixos) e o de GLS (efeitos aleatórios)
- Quando $E(u|X) = 0$ ambos estimadores são consistentes, ou seja,
\[ \hat{q} \equiv \hat{\beta}_{GLS} - \hat{\beta}_W\ \overset{p}{\rightarrow}\ 0 \]
então é preferível usar o mais eficiente (GLS, pois usa ambas variações inter e intra-indivíduos).
- Se $E(u|X) \neq 0$, então $\hat{q} \equiv \hat{\beta}_{GLS} - \hat{\beta}_W \neq 0$ e apenas o modelo robusto a $E(u|X) \neq 0$ (_within_) é consistente.
- A variância é dada por 
\begin{align*}
    V(\hat{q}) &= V(\hat{\beta}_{GLS} - \hat{\beta}_W) = V(\hat{\beta}_{GLS}) + V(\hat{\beta}_W) - 2 cov(\hat{\beta}_{W}, \hat{\beta}_{GLS}) \\
    &= \sigma^2_\nu (Z' W Z)^{-1} - (Z'\Omega^{-1} Z)^{-1}
\end{align*}
- Logo, a estatística teste se torna
\[ \hat{q}'\ V(\hat{q})^{-1}\ \hat{q} \]
que, sob $H_0$, é distribuida como $\chi^2$ com $K$ graus de liberdade.

```{r}
# Teste de Hausman
phtest(Q.within, Q.gls)
```
Não se rejeita a hipótese nula de que ambos modelos são consistentes a 5\%.


### Outros testes
- Ver Croissant \& Millo (2018):
    - Abordagem de Mundlak
    - Abordagem de Chamberlain

